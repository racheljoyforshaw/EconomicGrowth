From 08a1fbdd03451a812a44320f34dea4727fe0444b Mon Sep 17 00:00:00 2001
From: Tom Augspurger <tom.w.augspurger@gmail.com>
Date: Wed, 31 Jul 2019 12:00:00 -0500
Subject: [PATCH 2/3] PATCH: update getting-started

---
 doc/source/getting_started/10min.rst          | 168 ++++-----
 doc/source/getting_started/basics.rst         | 318 +++++++++---------
 .../comparison/comparison_with_r.rst          |  30 +-
 .../comparison/comparison_with_sas.rst        |  50 +--
 .../comparison/comparison_with_sql.rst        |  60 ++--
 .../comparison/comparison_with_stata.rst      |  48 +--
 doc/source/getting_started/dsintro.rst        | 112 +++---
 7 files changed, 393 insertions(+), 393 deletions(-)

diff --git a/doc/source/getting_started/10min.rst b/doc/source/getting_started/10min.rst
index 510c7ef97..11d6b7b2d 100644
--- a/doc/source/getting_started/10min.rst
+++ b/doc/source/getting_started/10min.rst
@@ -11,7 +11,7 @@ You can see more complex recipes in the :ref:`Cookbook<cookbook>`.
 
 Customarily, we import as follows:
 
-.. ipython:: python
+.. code:: python
 
    import numpy as np
    import pandas as pd
@@ -24,7 +24,7 @@ See the :ref:`Data Structure Intro section <dsintro>`.
 Creating a :class:`Series` by passing a list of values, letting pandas create
 a default integer index:
 
-.. ipython:: python
+.. code:: python
 
    s = pd.Series([1, 3, 5, np.nan, 6, 8])
    s
@@ -32,7 +32,7 @@ a default integer index:
 Creating a :class:`DataFrame` by passing a NumPy array, with a datetime index
 and labeled columns:
 
-.. ipython:: python
+.. code:: python
 
    dates = pd.date_range('20130101', periods=6)
    dates
@@ -41,7 +41,7 @@ and labeled columns:
 
 Creating a ``DataFrame`` by passing a dict of objects that can be converted to series-like.
 
-.. ipython:: python
+.. code:: python
 
    df2 = pd.DataFrame({'A': 1.,
                        'B': pd.Timestamp('20130102'),
@@ -54,7 +54,7 @@ Creating a ``DataFrame`` by passing a dict of objects that can be converted to s
 The columns of the resulting ``DataFrame`` have different
 :ref:`dtypes <basics.dtypes>`.
 
-.. ipython:: python
+.. code:: python
 
    df2.dtypes
 
@@ -90,14 +90,14 @@ See the :ref:`Basics section <basics>`.
 
 Here is how to view the top and bottom rows of the frame:
 
-.. ipython:: python
+.. code:: python
 
    df.head()
    df.tail(3)
 
 Display the index, columns:
 
-.. ipython:: python
+.. code:: python
 
    df.index
    df.columns
@@ -114,14 +114,14 @@ casting every value to a Python object.
 For ``df``, our :class:`DataFrame` of all floating-point values,
 :meth:`DataFrame.to_numpy` is fast and doesn't require copying data.
 
-.. ipython:: python
+.. code:: python
 
    df.to_numpy()
 
 For ``df2``, the :class:`DataFrame` with multiple dtypes,
 :meth:`DataFrame.to_numpy` is relatively expensive.
 
-.. ipython:: python
+.. code:: python
 
    df2.to_numpy()
 
@@ -132,25 +132,25 @@ For ``df2``, the :class:`DataFrame` with multiple dtypes,
 
 :func:`~DataFrame.describe` shows a quick statistic summary of your data:
 
-.. ipython:: python
+.. code:: python
 
    df.describe()
 
 Transposing your data:
 
-.. ipython:: python
+.. code:: python
 
    df.T
 
 Sorting by an axis:
 
-.. ipython:: python
+.. code:: python
 
    df.sort_index(axis=1, ascending=False)
 
 Sorting by values:
 
-.. ipython:: python
+.. code:: python
 
    df.sort_values(by='B')
 
@@ -172,13 +172,13 @@ Getting
 Selecting a single column, which yields a ``Series``,
 equivalent to ``df.A``:
 
-.. ipython:: python
+.. code:: python
 
    df['A']
 
 Selecting via ``[]``, which slices the rows.
 
-.. ipython:: python
+.. code:: python
 
    df[0:3]
    df['20130102':'20130104']
@@ -190,37 +190,37 @@ See more in :ref:`Selection by Label <indexing.label>`.
 
 For getting a cross section using a label:
 
-.. ipython:: python
+.. code:: python
 
    df.loc[dates[0]]
 
 Selecting on a multi-axis by label:
 
-.. ipython:: python
+.. code:: python
 
    df.loc[:, ['A', 'B']]
 
 Showing label slicing, both endpoints are *included*:
 
-.. ipython:: python
+.. code:: python
 
    df.loc['20130102':'20130104', ['A', 'B']]
 
 Reduction in the dimensions of the returned object:
 
-.. ipython:: python
+.. code:: python
 
    df.loc['20130102', ['A', 'B']]
 
 For getting a scalar value:
 
-.. ipython:: python
+.. code:: python
 
    df.loc[dates[0], 'A']
 
 For getting fast access to a scalar (equivalent to the prior method):
 
-.. ipython:: python
+.. code:: python
 
    df.at[dates[0], 'A']
 
@@ -231,43 +231,43 @@ See more in :ref:`Selection by Position <indexing.integer>`.
 
 Select via the position of the passed integers:
 
-.. ipython:: python
+.. code:: python
 
    df.iloc[3]
 
 By integer slices, acting similar to numpy/python:
 
-.. ipython:: python
+.. code:: python
 
    df.iloc[3:5, 0:2]
 
 By lists of integer position locations, similar to the numpy/python style:
 
-.. ipython:: python
+.. code:: python
 
    df.iloc[[1, 2, 4], [0, 2]]
 
 For slicing rows explicitly:
 
-.. ipython:: python
+.. code:: python
 
    df.iloc[1:3, :]
 
 For slicing columns explicitly:
 
-.. ipython:: python
+.. code:: python
 
    df.iloc[:, 1:3]
 
 For getting a value explicitly:
 
-.. ipython:: python
+.. code:: python
 
    df.iloc[1, 1]
 
 For getting fast access to a scalar (equivalent to the prior method):
 
-.. ipython:: python
+.. code:: python
 
    df.iat[1, 1]
 
@@ -276,19 +276,19 @@ Boolean indexing
 
 Using a single column's values to select data.
 
-.. ipython:: python
+.. code:: python
 
    df[df.A > 0]
 
 Selecting values from a DataFrame where a boolean condition is met.
 
-.. ipython:: python
+.. code:: python
 
    df[df > 0]
 
 Using the :func:`~Series.isin` method for filtering:
 
-.. ipython:: python
+.. code:: python
 
    df2 = df.copy()
    df2['E'] = ['one', 'one', 'two', 'three', 'four', 'three']
@@ -301,7 +301,7 @@ Setting
 Setting a new column automatically aligns the data
 by the indexes.
 
-.. ipython:: python
+.. code:: python
 
    s1 = pd.Series([1, 2, 3, 4, 5, 6], index=pd.date_range('20130102', periods=6))
    s1
@@ -309,31 +309,31 @@ by the indexes.
 
 Setting values by label:
 
-.. ipython:: python
+.. code:: python
 
    df.at[dates[0], 'A'] = 0
 
 Setting values by position:
 
-.. ipython:: python
+.. code:: python
 
    df.iat[0, 1] = 0
 
 Setting by assigning with a NumPy array:
 
-.. ipython:: python
+.. code:: python
 
    df.loc[:, 'D'] = np.array([5] * len(df))
 
 The result of the prior setting operations.
 
-.. ipython:: python
+.. code:: python
 
    df
 
 A ``where`` operation with setting.
 
-.. ipython:: python
+.. code:: python
 
    df2 = df.copy()
    df2[df2 > 0] = -df2
@@ -350,7 +350,7 @@ default not included in computations. See the :ref:`Missing Data section
 Reindexing allows you to change/add/delete the index on a specified axis. This
 returns a copy of the data.
 
-.. ipython:: python
+.. code:: python
 
    df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + ['E'])
    df1.loc[dates[0]:dates[1], 'E'] = 1
@@ -358,19 +358,19 @@ returns a copy of the data.
 
 To drop any rows that have missing data.
 
-.. ipython:: python
+.. code:: python
 
    df1.dropna(how='any')
 
 Filling missing data.
 
-.. ipython:: python
+.. code:: python
 
    df1.fillna(value=5)
 
 To get the boolean mask where values are ``nan``.
 
-.. ipython:: python
+.. code:: python
 
    pd.isna(df1)
 
@@ -387,20 +387,20 @@ Operations in general *exclude* missing data.
 
 Performing a descriptive statistic:
 
-.. ipython:: python
+.. code:: python
 
    df.mean()
 
 Same operation on the other axis:
 
-.. ipython:: python
+.. code:: python
 
    df.mean(1)
 
 Operating with objects that have different dimensionality and need alignment.
 In addition, pandas automatically broadcasts along the specified dimension.
 
-.. ipython:: python
+.. code:: python
 
    s = pd.Series([1, 3, 5, np.nan, 6, 8], index=dates).shift(2)
    s
@@ -412,7 +412,7 @@ Apply
 
 Applying functions to the data:
 
-.. ipython:: python
+.. code:: python
 
    df.apply(np.cumsum)
    df.apply(lambda x: x.max() - x.min())
@@ -422,7 +422,7 @@ Histogramming
 
 See more at :ref:`Histogramming and Discretization <basics.discretization>`.
 
-.. ipython:: python
+.. code:: python
 
    s = pd.Series(np.random.randint(0, 7, size=10))
    s
@@ -438,7 +438,7 @@ expressions <https://docs.python.org/3/library/re.html>`__ by default (and in
 some cases always uses them). See more at :ref:`Vectorized String Methods
 <text.string_methods>`.
 
-.. ipython:: python
+.. code:: python
 
    s = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca', np.nan, 'CABA', 'dog', 'cat'])
    s.str.lower()
@@ -458,7 +458,7 @@ See the :ref:`Merging section <merging>`.
 
 Concatenating pandas objects together with :func:`concat`:
 
-.. ipython:: python
+.. code:: python
 
    df = pd.DataFrame(np.random.randn(10, 4))
    df
@@ -473,7 +473,7 @@ Join
 
 SQL style merges. See the :ref:`Database style joining <merging.join>` section.
 
-.. ipython:: python
+.. code:: python
 
    left = pd.DataFrame({'key': ['foo', 'foo'], 'lval': [1, 2]})
    right = pd.DataFrame({'key': ['foo', 'foo'], 'rval': [4, 5]})
@@ -483,7 +483,7 @@ SQL style merges. See the :ref:`Database style joining <merging.join>` section.
 
 Another example that can be given is:
 
-.. ipython:: python
+.. code:: python
 
    left = pd.DataFrame({'key': ['foo', 'bar'], 'lval': [1, 2]})
    right = pd.DataFrame({'key': ['foo', 'bar'], 'rval': [4, 5]})
@@ -498,7 +498,7 @@ Append
 Append rows to a dataframe. See the :ref:`Appending <merging.concatenation>`
 section.
 
-.. ipython:: python
+.. code:: python
 
    df = pd.DataFrame(np.random.randn(8, 4), columns=['A', 'B', 'C', 'D'])
    df
@@ -518,7 +518,7 @@ following steps:
 
 See the :ref:`Grouping section <groupby>`.
 
-.. ipython:: python
+.. code:: python
 
    df = pd.DataFrame({'A': ['foo', 'bar', 'foo', 'bar',
                             'foo', 'bar', 'foo', 'foo'],
@@ -531,14 +531,14 @@ See the :ref:`Grouping section <groupby>`.
 Grouping and then applying the :meth:`~DataFrame.sum` function to the resulting
 groups.
 
-.. ipython:: python
+.. code:: python
 
    df.groupby('A').sum()
 
 Grouping by multiple columns forms a hierarchical index, and again we can
 apply the ``sum`` function.
 
-.. ipython:: python
+.. code:: python
 
    df.groupby(['A', 'B']).sum()
 
@@ -551,7 +551,7 @@ See the sections on :ref:`Hierarchical Indexing <advanced.hierarchical>` and
 Stack
 ~~~~~
 
-.. ipython:: python
+.. code:: python
 
    tuples = list(zip(*[['bar', 'bar', 'baz', 'baz',
                         'foo', 'foo', 'qux', 'qux'],
@@ -565,7 +565,7 @@ Stack
 The :meth:`~DataFrame.stack` method "compresses" a level in the DataFrame's
 columns.
 
-.. ipython:: python
+.. code:: python
 
    stacked = df2.stack()
    stacked
@@ -574,7 +574,7 @@ With a "stacked" DataFrame or Series (having a ``MultiIndex`` as the
 ``index``), the inverse operation of :meth:`~DataFrame.stack` is
 :meth:`~DataFrame.unstack`, which by default unstacks the **last level**:
 
-.. ipython:: python
+.. code:: python
 
    stacked.unstack()
    stacked.unstack(1)
@@ -584,7 +584,7 @@ Pivot tables
 ~~~~~~~~~~~~
 See the section on :ref:`Pivot Tables <reshaping.pivot>`.
 
-.. ipython:: python
+.. code:: python
 
    df = pd.DataFrame({'A': ['one', 'one', 'two', 'three'] * 3,
                       'B': ['A', 'B', 'C'] * 4,
@@ -595,7 +595,7 @@ See the section on :ref:`Pivot Tables <reshaping.pivot>`.
 
 We can produce pivot tables from this data very easily:
 
-.. ipython:: python
+.. code:: python
 
    pd.pivot_table(df, values='D', index=['A', 'B'], columns=['C'])
 
@@ -608,7 +608,7 @@ resampling operations during frequency conversion (e.g., converting secondly
 data into 5-minutely data). This is extremely common in, but not limited to,
 financial applications. See the :ref:`Time Series section <timeseries>`.
 
-.. ipython:: python
+.. code:: python
 
    rng = pd.date_range('1/1/2012', periods=100, freq='S')
    ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)
@@ -616,7 +616,7 @@ financial applications. See the :ref:`Time Series section <timeseries>`.
 
 Time zone representation:
 
-.. ipython:: python
+.. code:: python
 
    rng = pd.date_range('3/6/2012 00:00', periods=5, freq='D')
    ts = pd.Series(np.random.randn(len(rng)), rng)
@@ -626,13 +626,13 @@ Time zone representation:
 
 Converting to another time zone:
 
-.. ipython:: python
+.. code:: python
 
    ts_utc.tz_convert('US/Eastern')
 
 Converting between time span representations:
 
-.. ipython:: python
+.. code:: python
 
    rng = pd.date_range('1/1/2012', periods=5, freq='M')
    ts = pd.Series(np.random.randn(len(rng)), index=rng)
@@ -646,7 +646,7 @@ functions to be used. In the following example, we convert a quarterly
 frequency with year ending in November to 9am of the end of the month following
 the quarter end:
 
-.. ipython:: python
+.. code:: python
 
    prng = pd.period_range('1990Q1', '2000Q4', freq='Q-NOV')
    ts = pd.Series(np.random.randn(len(prng)), prng)
@@ -659,14 +659,14 @@ Categoricals
 pandas can include categorical data in a ``DataFrame``. For full docs, see the
 :ref:`categorical introduction <categorical>` and the :ref:`API documentation <api.arrays.categorical>`.
 
-.. ipython:: python
+.. code:: python
 
     df = pd.DataFrame({"id": [1, 2, 3, 4, 5, 6],
                        "raw_grade": ['a', 'b', 'b', 'a', 'a', 'e']})
 
 Convert the raw grades to a categorical data type.
 
-.. ipython:: python
+.. code:: python
 
     df["grade"] = df["raw_grade"].astype("category")
     df["grade"]
@@ -674,14 +674,14 @@ Convert the raw grades to a categorical data type.
 Rename the categories to more meaningful names (assigning to
 ``Series.cat.categories`` is inplace!).
 
-.. ipython:: python
+.. code:: python
 
     df["grade"].cat.categories = ["very good", "good", "very bad"]
 
 Reorder the categories and simultaneously add the missing categories (methods under ``Series
 .cat`` return a new ``Series`` by default).
 
-.. ipython:: python
+.. code:: python
 
     df["grade"] = df["grade"].cat.set_categories(["very bad", "bad", "medium",
                                                   "good", "very good"])
@@ -689,13 +689,13 @@ Reorder the categories and simultaneously add the missing categories (methods un
 
 Sorting is per order in the categories, not lexical order.
 
-.. ipython:: python
+.. code:: python
 
     df.sort_values(by="grade")
 
 Grouping by a categorical column also shows empty categories.
 
-.. ipython:: python
+.. code:: python
 
     df.groupby("grade").size()
 
@@ -705,25 +705,25 @@ Plotting
 
 See the :ref:`Plotting <visualization>` docs.
 
-.. ipython:: python
+.. code:: python
    :suppress:
 
    import matplotlib.pyplot as plt
    plt.close('all')
 
-.. ipython:: python
+.. code:: python
 
    ts = pd.Series(np.random.randn(1000),
                   index=pd.date_range('1/1/2000', periods=1000))
    ts = ts.cumsum()
 
-   @savefig series_plot_basic.png
+
    ts.plot()
 
 On a DataFrame, the :meth:`~DataFrame.plot` method is a convenience to plot all
 of the columns with labels:
 
-.. ipython:: python
+.. code:: python
 
    df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index,
                      columns=['A', 'B', 'C', 'D'])
@@ -731,7 +731,7 @@ of the columns with labels:
 
    plt.figure()
    df.plot()
-   @savefig frame_plot_basic.png
+
    plt.legend(loc='best')
 
 Getting data in/out
@@ -742,17 +742,17 @@ CSV
 
 :ref:`Writing to a csv file. <io.store_in_csv>`
 
-.. ipython:: python
+.. code:: python
 
    df.to_csv('foo.csv')
 
 :ref:`Reading from a csv file. <io.read_csv_table>`
 
-.. ipython:: python
+.. code:: python
 
    pd.read_csv('foo.csv')
 
-.. ipython:: python
+.. code:: python
    :suppress:
 
    import os
@@ -765,17 +765,17 @@ Reading and writing to :ref:`HDFStores <io.hdf5>`.
 
 Writing to a HDF5 Store.
 
-.. ipython:: python
+.. code:: python
 
    df.to_hdf('foo.h5', 'df')
 
 Reading from a HDF5 Store.
 
-.. ipython:: python
+.. code:: python
 
    pd.read_hdf('foo.h5', 'df')
 
-.. ipython:: python
+.. code:: python
    :suppress:
 
    os.remove('foo.h5')
@@ -787,17 +787,17 @@ Reading and writing to :ref:`MS Excel <io.excel>`.
 
 Writing to an excel file.
 
-.. ipython:: python
+.. code:: python
 
    df.to_excel('foo.xlsx', sheet_name='Sheet1')
 
 Reading from an excel file.
 
-.. ipython:: python
+.. code:: python
 
    pd.read_excel('foo.xlsx', 'Sheet1', index_col=None, na_values=['NA'])
 
-.. ipython:: python
+.. code:: python
    :suppress:
 
    os.remove('foo.xlsx')
diff --git a/doc/source/getting_started/basics.rst b/doc/source/getting_started/basics.rst
index bc3b7b4c7..86e74c1cd 100644
--- a/doc/source/getting_started/basics.rst
+++ b/doc/source/getting_started/basics.rst
@@ -10,7 +10,7 @@ Here we discuss a lot of the essential functionality common to the pandas data
 structures. Here's how to create some of the objects used in the examples from
 the previous section:
 
-.. ipython:: python
+.. code:: python
 
    index = pd.date_range('1/1/2000', periods=8)
    s = pd.Series(np.random.randn(5), index=['a', 'b', 'c', 'd', 'e'])
@@ -26,7 +26,7 @@ To view a small sample of a Series or DataFrame object, use the
 :meth:`~DataFrame.head` and :meth:`~DataFrame.tail` methods. The default number
 of elements to display is five, but you may pass a custom number.
 
-.. ipython:: python
+.. code:: python
 
    long_series = pd.Series(np.random.randn(1000))
    long_series.head()
@@ -46,7 +46,7 @@ pandas objects have a number of attributes enabling you to access the metadata
 
 Note, **these attributes can be safely assigned to**!
 
-.. ipython:: python
+.. code:: python
 
    df[:2]
    df.columns = [x.lower() for x in df.columns]
@@ -62,7 +62,7 @@ NumPy's type system to add support for custom arrays
 To get the actual data inside a :class:`Index` or :class:`Series`, use
 the ``.array`` property
 
-.. ipython:: python
+.. code:: python
 
    s.array
    s.index.array
@@ -74,7 +74,7 @@ beyond the scope of this introduction. See :ref:`basics.dtypes` for more.
 If you know you need a NumPy array, use :meth:`~Series.to_numpy`
 or :meth:`numpy.asarray`.
 
-.. ipython:: python
+.. code:: python
 
    s.to_numpy()
    np.asarray(s)
@@ -95,14 +95,14 @@ are two possibly useful representations:
 
 Timezones may be preserved with ``dtype=object``
 
-.. ipython:: python
+.. code:: python
 
    ser = pd.Series(pd.date_range('2000', periods=2, tz="CET"))
    ser.to_numpy(dtype=object)
 
 Or thrown away with ``dtype='datetime64[ns]'``
 
-.. ipython:: python
+.. code:: python
 
    ser.to_numpy(dtype="datetime64[ns]")
 
@@ -110,7 +110,7 @@ Getting the "raw data" inside a :class:`DataFrame` is possibly a bit more
 complex. When your ``DataFrame`` only has a single data type for all the
 columns, :meth:`DataFrame.to_numpy` will return the underlying data:
 
-.. ipython:: python
+.. code:: python
 
    df.to_numpy()
 
@@ -204,7 +204,7 @@ for carrying out binary operations. For broadcasting behavior,
 Series input is of primary interest. Using these functions, you can use to
 either match on the *index* or *columns* via the **axis** keyword:
 
-.. ipython:: python
+.. code:: python
 
    df = pd.DataFrame({
        'one': pd.Series(np.random.randn(3), index=['a', 'b', 'c']),
@@ -220,14 +220,14 @@ either match on the *index* or *columns* via the **axis** keyword:
    df.sub(column, axis='index')
    df.sub(column, axis=0)
 
-.. ipython:: python
+.. code:: python
    :suppress:
 
    df_orig = df
 
 Furthermore you can align a level of a MultiIndexed DataFrame with a Series.
 
-.. ipython:: python
+.. code:: python
 
    dfmi = df.copy()
    dfmi.index = pd.MultiIndex.from_tuples([(1, 'a'), (1, 'b'),
@@ -239,7 +239,7 @@ Series and Index also support the :func:`divmod` builtin. This function takes
 the floor division and modulo operation at the same time returning a two-tuple
 of the same type as the left hand side. For example:
 
-.. ipython:: python
+.. code:: python
 
    s = pd.Series(np.arange(10))
    s
@@ -255,7 +255,7 @@ of the same type as the left hand side. For example:
 
 We can also do elementwise :func:`divmod`:
 
-.. ipython:: python
+.. code:: python
 
    div, rem = divmod(s, [2, 2, 3, 3, 4, 4, 5, 5, 6, 6])
    div
@@ -271,13 +271,13 @@ wish to treat NaN as 0 unless both DataFrames are missing that value, in which
 case the result will be NaN (you can later replace NaN with some other value
 using ``fillna`` if you wish).
 
-.. ipython:: python
+.. code:: python
    :suppress:
 
    df2 = df.copy()
    df2['three']['a'] = 1.
 
-.. ipython:: python
+.. code:: python
 
    df
    df2
@@ -293,7 +293,7 @@ Series and DataFrame have the binary comparison methods ``eq``, ``ne``, ``lt``,
 ``le``, and ``ge`` whose behavior is analogous to the binary
 arithmetic operations described above:
 
-.. ipython:: python
+.. code:: python
 
    df.gt(df2)
    df2.ne(df)
@@ -311,20 +311,20 @@ You can apply the reductions: :attr:`~DataFrame.empty`, :meth:`~DataFrame.any`,
 :meth:`~DataFrame.all`, and :meth:`~DataFrame.bool` to provide a
 way to summarize a boolean result.
 
-.. ipython:: python
+.. code:: python
 
    (df > 0).all()
    (df > 0).any()
 
 You can reduce to a final boolean value.
 
-.. ipython:: python
+.. code:: python
 
    (df > 0).any().any()
 
 You can test if a pandas object is empty, via the :attr:`~DataFrame.empty` property.
 
-.. ipython:: python
+.. code:: python
 
    df.empty
    pd.DataFrame(columns=list('ABC')).empty
@@ -332,7 +332,7 @@ You can test if a pandas object is empty, via the :attr:`~DataFrame.empty` prope
 To evaluate single-element pandas objects in a boolean context, use the method
 :meth:`~DataFrame.bool`:
 
-.. ipython:: python
+.. code:: python
 
    pd.Series([True]).bool()
    pd.Series([False]).bool()
@@ -371,7 +371,7 @@ that these two computations produce the same result, given the tools
 shown above, you might imagine using ``(df + df == df * 2).all()``. But in
 fact, this expression is False:
 
-.. ipython:: python
+.. code:: python
 
    df + df == df * 2
    (df + df == df * 2).all()
@@ -379,7 +379,7 @@ fact, this expression is False:
 Notice that the boolean DataFrame ``df + df == df * 2`` contains some False values!
 This is because NaNs do not compare as equals:
 
-.. ipython:: python
+.. code:: python
 
    np.nan == np.nan
 
@@ -387,14 +387,14 @@ So, NDFrames (such as Series and DataFrames)
 have an :meth:`~DataFrame.equals` method for testing equality, with NaNs in
 corresponding locations treated as equal.
 
-.. ipython:: python
+.. code:: python
 
    (df + df).equals(df * 2)
 
 Note that the Series or DataFrame index needs to be in the same order for
 equality to be True:
 
-.. ipython:: python
+.. code:: python
 
    df1 = pd.DataFrame({'col': ['foo', 0, np.nan]})
    df2 = pd.DataFrame({'col': [np.nan, 0, 'foo']}, index=[2, 1, 0])
@@ -407,7 +407,7 @@ Comparing array-like objects
 You can conveniently perform element-wise comparisons when comparing a pandas
 data structure with a scalar value:
 
-.. ipython:: python
+.. code:: python
 
    pd.Series(['foo', 'bar', 'baz']) == 'foo'
    pd.Index(['foo', 'bar', 'baz']) == 'foo'
@@ -415,7 +415,7 @@ data structure with a scalar value:
 Pandas also handles element-wise comparisons between different array-like
 objects of the same length:
 
-.. ipython:: python
+.. code:: python
 
     pd.Series(['foo', 'bar', 'baz']) == pd.Index(['foo', 'bar', 'qux'])
     pd.Series(['foo', 'bar', 'baz']) == np.array(['foo', 'bar', 'qux'])
@@ -434,13 +434,13 @@ raise a ValueError:
 Note that this is different from the NumPy behavior where a comparison can
 be broadcast:
 
-.. ipython:: python
+.. code:: python
 
     np.array([1, 2, 3]) == np.array([2])
 
 or it can return False if broadcasting can not be done:
 
-.. ipython:: python
+.. code:: python
    :okwarning:
 
     np.array([1, 2, 3]) == np.array([1, 2])
@@ -458,7 +458,7 @@ conditionally filled with like-labeled values from the other DataFrame. The
 function implementing this operation is :meth:`~DataFrame.combine_first`,
 which we illustrate:
 
-.. ipython:: python
+.. code:: python
 
    df1 = pd.DataFrame({'A': [1., np.nan, 3., 5., np.nan],
                        'B': [np.nan, 2., 3., np.nan, 6.]})
@@ -478,7 +478,7 @@ function pairs of Series (i.e., columns whose names are the same).
 
 So, for instance, to reproduce :meth:`~DataFrame.combine_first` as above:
 
-.. ipython:: python
+.. code:: python
 
    def combiner(x, y):
        return np.where(pd.isna(x), y, x)
@@ -504,7 +504,7 @@ specified by name or integer:
 
 For example:
 
-.. ipython:: python
+.. code:: python
 
    df
    df.mean(0)
@@ -513,7 +513,7 @@ For example:
 All such methods have a ``skipna`` option signaling whether to exclude missing
 data (``True`` by default):
 
-.. ipython:: python
+.. code:: python
 
    df.sum(0, skipna=False)
    df.sum(axis=1, skipna=True)
@@ -522,7 +522,7 @@ Combined with the broadcasting / arithmetic behavior, one can describe various
 statistical procedures, like standardization (rendering data zero mean and
 standard deviation 1), very concisely:
 
-.. ipython:: python
+.. code:: python
 
    ts_stand = (df - df.mean()) / df.std()
    ts_stand.std()
@@ -534,7 +534,7 @@ preserve the location of ``NaN`` values. This is somewhat different from
 :meth:`~DataFrame.expanding` and :meth:`~DataFrame.rolling`.
 For more details please see :ref:`this note <stats.moments.expanding.note>`.
 
-.. ipython:: python
+.. code:: python
 
    df.cumsum()
 
@@ -570,7 +570,7 @@ optional ``level`` parameter which applies only if the object has a
 Note that by chance some NumPy methods, like ``mean``, ``std``, and ``sum``,
 will exclude NAs on Series input by default:
 
-.. ipython:: python
+.. code:: python
 
    np.mean(df['one'])
    np.mean(df['one'].to_numpy())
@@ -578,7 +578,7 @@ will exclude NAs on Series input by default:
 :meth:`Series.nunique` will return the number of unique non-NA values in a
 Series:
 
-.. ipython:: python
+.. code:: python
 
    series = pd.Series(np.random.randn(500))
    series[20:500] = np.nan
@@ -594,7 +594,7 @@ There is a convenient :meth:`~DataFrame.describe` function which computes a vari
 statistics about a Series or the columns of a DataFrame (excluding NAs of
 course):
 
-.. ipython:: python
+.. code:: python
 
     series = pd.Series(np.random.randn(1000))
     series[::2] = np.nan
@@ -606,7 +606,7 @@ course):
 
 You can select specific percentiles to include in the output:
 
-.. ipython:: python
+.. code:: python
 
     series.describe(percentiles=[.05, .25, .75, .95])
 
@@ -615,7 +615,7 @@ By default, the median is always included.
 For a non-numerical Series object, :meth:`~Series.describe` will give a simple
 summary of the number of unique values and most frequently occurring values:
 
-.. ipython:: python
+.. code:: python
 
    s = pd.Series(['a', 'a', 'b', 'b', 'a', 'a', np.nan, 'c', 'd', 'a'])
    s.describe()
@@ -624,7 +624,7 @@ Note that on a mixed-type DataFrame object, :meth:`~DataFrame.describe` will
 restrict the summary to include only numerical columns or, if none are, only
 categorical columns:
 
-.. ipython:: python
+.. code:: python
 
     frame = pd.DataFrame({'a': ['Yes', 'Yes', 'No', 'No'], 'b': range(4)})
     frame.describe()
@@ -632,7 +632,7 @@ categorical columns:
 This behavior can be controlled by providing a list of types as ``include``/``exclude``
 arguments. The special value ``all`` can also be used:
 
-.. ipython:: python
+.. code:: python
 
     frame.describe(include=['object'])
     frame.describe(include=['number'])
@@ -650,7 +650,7 @@ The :meth:`~DataFrame.idxmin` and :meth:`~DataFrame.idxmax` functions on Series
 and DataFrame compute the index labels with the minimum and maximum
 corresponding values:
 
-.. ipython:: python
+.. code:: python
 
    s1 = pd.Series(np.random.randn(5))
    s1
@@ -665,7 +665,7 @@ When there are multiple rows (or columns) matching the minimum or maximum
 value, :meth:`~DataFrame.idxmin` and :meth:`~DataFrame.idxmax` return the first
 matching index:
 
-.. ipython:: python
+.. code:: python
 
    df3 = pd.DataFrame([2, 1, 1, 3, np.nan], columns=['A'], index=list('edcba'))
    df3
@@ -683,7 +683,7 @@ Value counts (histogramming) / mode
 The :meth:`~Series.value_counts` Series method and top-level function computes a histogram
 of a 1D array of values. It can also be used as a function on regular arrays:
 
-.. ipython:: python
+.. code:: python
 
    data = np.random.randint(0, 7, size=50)
    data
@@ -693,7 +693,7 @@ of a 1D array of values. It can also be used as a function on regular arrays:
 
 Similarly, you can get the most frequently occurring value(s) (the mode) of the values in a Series or DataFrame:
 
-.. ipython:: python
+.. code:: python
 
     s5 = pd.Series([1, 1, 3, 3, 3, 5, 5, 7, 7, 7])
     s5.mode()
@@ -708,7 +708,7 @@ Discretization and quantiling
 Continuous values can be discretized using the :func:`cut` (bins based on values)
 and :func:`qcut` (bins based on sample quantiles) functions:
 
-.. ipython:: python
+.. code:: python
 
    arr = np.random.randn(20)
    factor = pd.cut(arr, 4)
@@ -720,7 +720,7 @@ and :func:`qcut` (bins based on sample quantiles) functions:
 :func:`qcut` computes sample quantiles. For example, we could slice up some
 normally distributed data into equal-size quartiles like so:
 
-.. ipython:: python
+.. code:: python
 
    arr = np.random.randn(30)
    factor = pd.qcut(arr, [0, .25, .5, .75, 1])
@@ -729,7 +729,7 @@ normally distributed data into equal-size quartiles like so:
 
 We can also pass infinite values to define the bins:
 
-.. ipython:: python
+.. code:: python
 
    arr = np.random.randn(20)
    factor = pd.cut(arr, [-np.inf, 0, np.inf])
@@ -783,7 +783,7 @@ In this case, provide ``pipe`` with a tuple of ``(callable, data_keyword)``.
 
 For example, we can fit a regression using statsmodels. Their API expects a formula first and a ``DataFrame`` as the second argument, ``data``. We pass in the function, keyword pair ``(sm.ols, 'data')`` to ``pipe``:
 
-.. ipython:: python
+.. code:: python
    :okwarning:
 
    import statsmodels.formula.api as sm
@@ -814,7 +814,7 @@ Arbitrary functions can be applied along the axes of a DataFrame
 using the :meth:`~DataFrame.apply` method, which, like the descriptive
 statistics methods, takes an optional ``axis`` argument:
 
-.. ipython:: python
+.. code:: python
 
    df.apply(np.mean)
    df.apply(np.mean, axis=1)
@@ -824,7 +824,7 @@ statistics methods, takes an optional ``axis`` argument:
 
 The :meth:`~DataFrame.apply` method will also dispatch on a string method name.
 
-.. ipython:: python
+.. code:: python
 
    df.apply('mean')
    df.apply('mean', axis=1)
@@ -844,7 +844,7 @@ These will determine how list-likes return values expand (or not) to a ``DataFra
 about a data set. For example, suppose we wanted to extract the date where the
 maximum value for each column occurred:
 
-.. ipython:: python
+.. code:: python
 
    tsdf = pd.DataFrame(np.random.randn(1000, 3), columns=['A', 'B', 'C'],
                        index=pd.date_range('1/1/2000', periods=1000))
@@ -867,14 +867,14 @@ You may then apply this function as follows:
 Another useful feature is the ability to pass Series methods to carry out some
 Series operation on each column or row:
 
-.. ipython:: python
+.. code:: python
    :suppress:
 
    tsdf = pd.DataFrame(np.random.randn(10, 3), columns=['A', 'B', 'C'],
                        index=pd.date_range('1/1/2000', periods=10))
    tsdf.iloc[3:7] = np.nan
 
-.. ipython:: python
+.. code:: python
 
    tsdf
    tsdf.apply(pd.Series.interpolate)
@@ -901,7 +901,7 @@ The entry point for aggregation is :meth:`DataFrame.aggregate`, or the alias
 
 We will use a similar starting frame from above:
 
-.. ipython:: python
+.. code:: python
 
    tsdf = pd.DataFrame(np.random.randn(10, 3), columns=['A', 'B', 'C'],
                        index=pd.date_range('1/1/2000', periods=10))
@@ -912,7 +912,7 @@ Using a single function is equivalent to :meth:`~DataFrame.apply`. You can also
 pass named methods as strings. These will return a ``Series`` of the aggregated
 output:
 
-.. ipython:: python
+.. code:: python
 
    tsdf.agg(np.sum)
 
@@ -924,7 +924,7 @@ output:
 
 Single aggregations on a ``Series`` this will return a scalar value:
 
-.. ipython:: python
+.. code:: python
 
    tsdf.A.agg('sum')
 
@@ -936,31 +936,31 @@ You can pass multiple aggregation arguments as a list.
 The results of each of the passed functions will be a row in the resulting ``DataFrame``.
 These are naturally named from the aggregation function.
 
-.. ipython:: python
+.. code:: python
 
    tsdf.agg(['sum'])
 
 Multiple functions yield multiple rows:
 
-.. ipython:: python
+.. code:: python
 
    tsdf.agg(['sum', 'mean'])
 
 On a ``Series``, multiple functions return a ``Series``, indexed by the function names:
 
-.. ipython:: python
+.. code:: python
 
    tsdf.A.agg(['sum', 'mean'])
 
 Passing a ``lambda`` function will yield a ``<lambda>`` named row:
 
-.. ipython:: python
+.. code:: python
 
    tsdf.A.agg(['sum', lambda x: x.mean()])
 
 Passing a named function will yield that name for the row:
 
-.. ipython:: python
+.. code:: python
 
    def mymean(x):
        return x.mean()
@@ -974,7 +974,7 @@ Passing a dictionary of column names to a scalar or a list of scalars, to ``Data
 allows you to customize which functions are applied to which columns. Note that the results
 are not in any particular order, you can use an ``OrderedDict`` instead to guarantee ordering.
 
-.. ipython:: python
+.. code:: python
 
    tsdf.agg({'A': 'mean', 'B': 'sum'})
 
@@ -982,7 +982,7 @@ Passing a list-like will generate a ``DataFrame`` output. You will get a matrix-
 of all of the aggregators. The output will consist of all unique functions. Those that are
 not noted for a particular column will be ``NaN``:
 
-.. ipython:: python
+.. code:: python
 
    tsdf.agg({'A': ['mean', 'min'], 'B': 'sum'})
 
@@ -994,7 +994,7 @@ Mixed dtypes
 When presented with mixed dtypes that cannot aggregate, ``.agg`` will only take the valid
 aggregations. This is similar to how groupby ``.agg`` works.
 
-.. ipython:: python
+.. code:: python
 
    mdf = pd.DataFrame({'A': [1, 2, 3],
                        'B': [1., 2., 3.],
@@ -1002,7 +1002,7 @@ aggregations. This is similar to how groupby ``.agg`` works.
                        'D': pd.date_range('20130101', periods=3)})
    mdf.dtypes
 
-.. ipython:: python
+.. code:: python
 
    mdf.agg(['min', 'sum'])
 
@@ -1014,7 +1014,7 @@ Custom describe
 With ``.agg()`` is it possible to easily create a custom describe function, similar
 to the built in :ref:`describe function <basics.describe>`.
 
-.. ipython:: python
+.. code:: python
 
    from functools import partial
 
@@ -1038,7 +1038,7 @@ time rather than one-by-one. Its API is quite similar to the ``.agg`` API.
 
 We create a frame similar to the one used in the above sections.
 
-.. ipython:: python
+.. code:: python
 
    tsdf = pd.DataFrame(np.random.randn(10, 3), columns=['A', 'B', 'C'],
                        index=pd.date_range('1/1/2000', periods=10))
@@ -1048,7 +1048,7 @@ We create a frame similar to the one used in the above sections.
 Transform the entire frame. ``.transform()`` allows input functions as: a NumPy function, a string
 function name or a user defined function.
 
-.. ipython:: python
+.. code:: python
    :okwarning:
 
    tsdf.transform(np.abs)
@@ -1057,13 +1057,13 @@ function name or a user defined function.
 
 Here :meth:`~DataFrame.transform` received a single function; this is equivalent to a ufunc application.
 
-.. ipython:: python
+.. code:: python
 
    np.abs(tsdf)
 
 Passing a single function to ``.transform()`` with a ``Series`` will yield a single ``Series`` in return.
 
-.. ipython:: python
+.. code:: python
 
    tsdf.A.transform(np.abs)
 
@@ -1075,14 +1075,14 @@ Passing multiple functions will yield a column MultiIndexed DataFrame.
 The first level will be the original frame column names; the second level
 will be the names of the transforming functions.
 
-.. ipython:: python
+.. code:: python
 
    tsdf.transform([np.abs, lambda x: x + 1])
 
 Passing multiple functions to a Series will yield a DataFrame. The
 resulting column names will be the transforming functions.
 
-.. ipython:: python
+.. code:: python
 
    tsdf.A.transform([np.abs, lambda x: x + 1])
 
@@ -1093,14 +1093,14 @@ Transforming with a dict
 
 Passing a dict of functions will allow selective transforming per column.
 
-.. ipython:: python
+.. code:: python
 
    tsdf.transform({'A': np.abs, 'B': lambda x: x + 1})
 
 Passing a dict of lists will generate a MultiIndexed DataFrame with these
 selective transforms.
 
-.. ipython:: python
+.. code:: python
    :okwarning:
 
    tsdf.transform({'A': np.abs, 'B': [lambda x: x + 1, 'sqrt']})
@@ -1115,12 +1115,12 @@ another array or value), the methods :meth:`~DataFrame.applymap` on DataFrame
 and analogously :meth:`~Series.map` on Series accept any Python function taking
 a single value and returning a single value. For example:
 
-.. ipython:: python
+.. code:: python
    :suppress:
 
    df4 = df_orig.copy()
 
-.. ipython:: python
+.. code:: python
 
    df4
 
@@ -1134,7 +1134,7 @@ a single value and returning a single value. For example:
 "link" or "map" values defined by a secondary series. This is closely related
 to :ref:`merging/joining functionality <merging>`:
 
-.. ipython:: python
+.. code:: python
 
    s = pd.Series(['six', 'seven', 'six', 'seven', 'six'],
                  index=['a', 'b', 'c', 'd', 'e'])
@@ -1161,7 +1161,7 @@ labels along a particular axis. This accomplishes several things:
 
 Here is a simple example:
 
-.. ipython:: python
+.. code:: python
 
    s = pd.Series(np.random.randn(5), index=['a', 'b', 'c', 'd', 'e'])
    s
@@ -1172,14 +1172,14 @@ Here, the ``f`` label was not contained in the Series and hence appears as
 
 With a DataFrame, you can simultaneously reindex the index and columns:
 
-.. ipython:: python
+.. code:: python
 
    df
    df.reindex(index=['c', 'f', 'b'], columns=['three', 'two', 'one'])
 
 You may also use ``reindex`` with an ``axis`` keyword:
 
-.. ipython:: python
+.. code:: python
 
    df.reindex(['c', 'f', 'b'], axis='index')
 
@@ -1187,7 +1187,7 @@ Note that the ``Index`` objects containing the actual axis labels can be
 **shared** between objects. So if we have a Series and a DataFrame, the
 following can be done:
 
-.. ipython:: python
+.. code:: python
 
    rs = s.reindex(df.index)
    rs
@@ -1201,7 +1201,7 @@ DataFrame's index.
 :meth:`DataFrame.reindex` also supports an "axis-style" calling convention,
 where you specify a single ``labels`` argument and the ``axis`` it applies to.
 
-.. ipython:: python
+.. code:: python
 
    df.reindex(['c', 'f', 'b'], axis='index')
    df.reindex(['three', 'two', 'one'], axis='columns')
@@ -1231,14 +1231,14 @@ another object. While the syntax for this is straightforward albeit verbose, it
 is a common enough operation that the :meth:`~DataFrame.reindex_like` method is
 available to make this simpler:
 
-.. ipython:: python
+.. code:: python
    :suppress:
 
    df2 = df.reindex(['a', 'b', 'c'], columns=['one', 'two'])
    df3 = df2 - df2.mean()
 
 
-.. ipython:: python
+.. code:: python
 
    df2
    df3
@@ -1259,7 +1259,7 @@ supports a ``join`` argument (related to :ref:`joining and merging <merging>`):
 
 It returns a tuple with both of the reindexed Series:
 
-.. ipython:: python
+.. code:: python
 
    s = pd.Series(np.random.randn(5), index=['a', 'b', 'c', 'd', 'e'])
    s1 = s[:4]
@@ -1273,13 +1273,13 @@ It returns a tuple with both of the reindexed Series:
 For DataFrames, the join method will be applied to both the index and the
 columns by default:
 
-.. ipython:: python
+.. code:: python
 
    df.align(df2, join='inner')
 
 You can also pass an ``axis`` option to only align on the specified axis:
 
-.. ipython:: python
+.. code:: python
 
    df.align(df2, join='inner', axis=0)
 
@@ -1288,7 +1288,7 @@ You can also pass an ``axis`` option to only align on the specified axis:
 If you pass a Series to :meth:`DataFrame.align`, you can choose to align both
 objects either on the DataFrame's index or columns using the ``axis`` argument:
 
-.. ipython:: python
+.. code:: python
 
    df.align(df2.iloc[0], axis=1)
 
@@ -1310,7 +1310,7 @@ filling method chosen from the following table:
 
 We illustrate these fill methods on a simple Series:
 
-.. ipython:: python
+.. code:: python
 
    rng = pd.date_range('1/3/2000', periods=8)
    ts = pd.Series(np.random.randn(8), index=rng)
@@ -1330,7 +1330,7 @@ Note that the same result could have been achieved using
 :ref:`fillna <missing_data.fillna>` (except for ``method='nearest'``) or
 :ref:`interpolate <missing_data.interpolate>`:
 
-.. ipython:: python
+.. code:: python
 
    ts2.reindex(ts.index).fillna(method='ffill')
 
@@ -1347,14 +1347,14 @@ The ``limit`` and ``tolerance`` arguments provide additional control over
 filling while reindexing. Limit specifies the maximum count of consecutive
 matches:
 
-.. ipython:: python
+.. code:: python
 
    ts2.reindex(ts.index, method='ffill', limit=1)
 
 In contrast, tolerance specifies the maximum distance between the index and
 indexer values:
 
-.. ipython:: python
+.. code:: python
 
    ts2.reindex(ts.index, method='ffill', tolerance='1 day')
 
@@ -1370,7 +1370,7 @@ Dropping labels from an axis
 A method closely related to ``reindex`` is the :meth:`~DataFrame.drop` function.
 It removes a set of labels from an axis:
 
-.. ipython:: python
+.. code:: python
 
    df
    df.drop(['a', 'd'], axis=0)
@@ -1378,7 +1378,7 @@ It removes a set of labels from an axis:
 
 Note that the following also works, but is a bit less obvious / clean:
 
-.. ipython:: python
+.. code:: python
 
    df.reindex(df.index.difference(['a', 'd']))
 
@@ -1390,7 +1390,7 @@ Renaming / mapping labels
 The :meth:`~DataFrame.rename` method allows you to relabel an axis based on some
 mapping (a dict or Series) or an arbitrary function.
 
-.. ipython:: python
+.. code:: python
 
    s
    s.rename(str.upper)
@@ -1399,7 +1399,7 @@ If you pass a function, it must return a value when called with any of the
 labels (and must produce a set of unique values). A dict or
 Series can also be used:
 
-.. ipython:: python
+.. code:: python
 
    df.rename(columns={'one': 'foo', 'two': 'bar'},
              index={'a': 'apple', 'b': 'banana', 'd': 'durian'})
@@ -1412,7 +1412,7 @@ extra labels in the mapping don't throw an error.
 :meth:`DataFrame.rename` also supports an "axis-style" calling convention, where
 you specify a single ``mapper`` and the ``axis`` to apply that mapping to.
 
-.. ipython:: python
+.. code:: python
 
    df.rename({'one': 'foo', 'two': 'bar'}, axis='columns')
    df.rename({'a': 'apple', 'b': 'banana', 'd': 'durian'}, axis='index')
@@ -1427,7 +1427,7 @@ parameter that is by default ``False`` and copies the underlying data. Pass
 Finally, :meth:`~Series.rename` also accepts a scalar or list-like
 for altering the ``Series.name`` attribute.
 
-.. ipython:: python
+.. code:: python
 
    s.rename("scalar-name")
 
@@ -1439,7 +1439,7 @@ The methods :meth:`~DataFrame.rename_axis` and :meth:`~Series.rename_axis`
 allow specific names of a `MultiIndex` to be changed (as opposed to the
 labels).
 
-.. ipython:: python
+.. code:: python
 
    df = pd.DataFrame({'x': [1, 2, 3, 4, 5, 6],
                       'y': [10, 20, 30, 40, 50, 60]},
@@ -1466,7 +1466,7 @@ In short, basic iteration (``for i in object``) produces:
 
 Thus, for example, iterating over a DataFrame gives you the column names:
 
-.. ipython:: python
+.. code:: python
 
    df = pd.DataFrame({'col1': np.random.randn(3),
                       'col2': np.random.randn(3)}, index=['a', 'b', 'c'])
@@ -1515,7 +1515,7 @@ To iterate over the rows of a DataFrame, you can use the following methods:
 
   For example, in the following case setting the value has no effect:
 
-  .. ipython:: python
+  .. code:: python
 
     df = pd.DataFrame({'a': [1, 2, 3], 'b': ['a', 'b', 'c']})
 
@@ -1535,7 +1535,7 @@ through key-value pairs:
 
 For example:
 
-.. ipython:: python
+.. code:: python
 
    for label, ser in df.items():
        print(label)
@@ -1550,7 +1550,7 @@ iterrows
 DataFrame as Series objects. It returns an iterator yielding each
 index value along with a Series containing the data in each row:
 
-.. ipython:: python
+.. code:: python
 
    for row_index, row in df.iterrows():
        print(row_index, row, sep='\n')
@@ -1561,7 +1561,7 @@ index value along with a Series containing the data in each row:
    it does **not** preserve dtypes across the rows (dtypes are
    preserved across columns for DataFrames). For example,
 
-   .. ipython:: python
+   .. code:: python
 
       df_orig = pd.DataFrame([[1, 1.5]], columns=['int', 'float'])
       df_orig.dtypes
@@ -1571,7 +1571,7 @@ index value along with a Series containing the data in each row:
    All values in ``row``, returned as a Series, are now upcasted
    to floats, also the original integer value in column `x`:
 
-   .. ipython:: python
+   .. code:: python
 
       row['int'].dtype
       df_orig['int'].dtype
@@ -1582,7 +1582,7 @@ index value along with a Series containing the data in each row:
 
 For instance, a contrived way to transpose the DataFrame would be:
 
-.. ipython:: python
+.. code:: python
 
    df2 = pd.DataFrame({'x': [1, 2, 3], 'y': [4, 5, 6]})
    print(df2)
@@ -1601,7 +1601,7 @@ remaining values are the row values.
 
 For instance:
 
-.. ipython:: python
+.. code:: python
 
    for row in df.itertuples():
        print(row)
@@ -1626,7 +1626,7 @@ and is generally faster as :meth:`~DataFrame.iterrows`.
 *values* of the Series, if it is a datetime/period like Series.
 This will return a Series, indexed like the existing Series.
 
-.. ipython:: python
+.. code:: python
 
    # datetime
    s = pd.Series(pd.date_range('20130101 09:10:12', periods=4))
@@ -1637,13 +1637,13 @@ This will return a Series, indexed like the existing Series.
 
 This enables nice expressions like this:
 
-.. ipython:: python
+.. code:: python
 
    s[s.dt.day == 2]
 
 You can easily produces tz aware transformations:
 
-.. ipython:: python
+.. code:: python
 
    stz = s.dt.tz_localize('US/Eastern')
    stz
@@ -1651,21 +1651,21 @@ You can easily produces tz aware transformations:
 
 You can also chain these types of operations:
 
-.. ipython:: python
+.. code:: python
 
    s.dt.tz_localize('UTC').dt.tz_convert('US/Eastern')
 
 You can also format datetime values as strings with :meth:`Series.dt.strftime` which
 supports the same format as the standard :meth:`~datetime.datetime.strftime`.
 
-.. ipython:: python
+.. code:: python
 
    # DatetimeIndex
    s = pd.Series(pd.date_range('20130101', periods=4))
    s
    s.dt.strftime('%Y/%m/%d')
 
-.. ipython:: python
+.. code:: python
 
    # PeriodIndex
    s = pd.Series(pd.period_range('20130101', periods=4))
@@ -1674,7 +1674,7 @@ supports the same format as the standard :meth:`~datetime.datetime.strftime`.
 
 The ``.dt`` accessor works for period and timedelta dtypes.
 
-.. ipython:: python
+.. code:: python
 
    # period
    s = pd.Series(pd.period_range('20130101', periods=4, freq='D'))
@@ -1682,7 +1682,7 @@ The ``.dt`` accessor works for period and timedelta dtypes.
    s.dt.year
    s.dt.day
 
-.. ipython:: python
+.. code:: python
 
    # timedelta
    s = pd.Series(pd.timedelta_range('1 day 00:00:05', periods=4, freq='s'))
@@ -1704,7 +1704,7 @@ exclude missing/NA values automatically. These are accessed via the Series's
 ``str`` attribute and generally have names matching the equivalent (scalar)
 built-in string methods. For example:
 
- .. ipython:: python
+ .. code:: python
 
   s = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca', np.nan, 'CABA', 'dog', 'cat'])
   s.str.lower()
@@ -1733,7 +1733,7 @@ By index
 The :meth:`Series.sort_index` and :meth:`DataFrame.sort_index` methods are
 used to sort a pandas object by its index levels.
 
-.. ipython:: python
+.. code:: python
 
    df = pd.DataFrame({
        'one': pd.Series(np.random.randn(3), index=['a', 'b', 'c']),
@@ -1762,7 +1762,7 @@ The :meth:`Series.sort_values` method is used to sort a `Series` by its values.
 The optional ``by`` parameter to :meth:`DataFrame.sort_values` may used to specify one or more columns
 to use to determine the sorted order.
 
-.. ipython:: python
+.. code:: python
 
    df1 = pd.DataFrame({'one': [2, 1, 1, 1],
                        'two': [1, 3, 2, 4],
@@ -1771,14 +1771,14 @@ to use to determine the sorted order.
 
 The ``by`` parameter can take a list of column names, e.g.:
 
-.. ipython:: python
+.. code:: python
 
    df1[['one', 'two', 'three']].sort_values(by=['one', 'two'])
 
 These methods have special treatment of NA values via the ``na_position``
 argument:
 
-.. ipython:: python
+.. code:: python
 
    s[2] = np.nan
    s.sort_values()
@@ -1794,7 +1794,7 @@ By indexes and values
 Strings passed as the ``by`` parameter to :meth:`DataFrame.sort_values` may
 refer to either columns or index level names.
 
-.. ipython:: python
+.. code:: python
 
    # Build MultiIndex
    idx = pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('a', 2),
@@ -1808,7 +1808,7 @@ refer to either columns or index level names.
 
 Sort by 'second' (index) and 'A' (column)
 
-.. ipython:: python
+.. code:: python
 
    df_multi.sort_values(by=['second', 'A'])
 
@@ -1826,7 +1826,7 @@ searchsorted
 Series has the :meth:`~Series.searchsorted` method, which works similarly to
 :meth:`numpy.ndarray.searchsorted`.
 
-.. ipython:: python
+.. code:: python
 
    ser = pd.Series([1, 2, 3])
    ser.searchsorted([0, 3])
@@ -1845,7 +1845,7 @@ smallest / largest values
 smallest or largest :math:`n` values. For a large ``Series`` this can be much
 faster than sorting the entire Series and calling ``head(n)`` on the result.
 
-.. ipython:: python
+.. code:: python
 
    s = pd.Series(np.random.permutation(10))
    s
@@ -1855,7 +1855,7 @@ faster than sorting the entire Series and calling ``head(n)`` on the result.
 
 ``DataFrame`` also has the ``nlargest`` and ``nsmallest`` methods.
 
-.. ipython:: python
+.. code:: python
 
    df = pd.DataFrame({'a': [-2, -1, 1, 10, 8, 11, -1],
                       'b': list('abdceff'),
@@ -1874,7 +1874,7 @@ Sorting by a MultiIndex column
 You must be explicit about sorting when the column is a MultiIndex, and fully specify
 all levels to ``by``.
 
-.. ipython:: python
+.. code:: python
 
    df1.columns = pd.MultiIndex.from_tuples([('a', 'one'),
                                             ('a', 'two'),
@@ -1938,7 +1938,7 @@ other libraries and methods. See :ref:`basics.object_conversion`).
 A convenient :attr:`~DataFrame.dtypes` attribute for DataFrame returns a Series
 with the data type of each column.
 
-.. ipython:: python
+.. code:: python
 
    dft = pd.DataFrame({'A': np.random.rand(3),
                        'B': 1,
@@ -1952,7 +1952,7 @@ with the data type of each column.
 
 On a ``Series`` object, use the :attr:`~Series.dtype` attribute.
 
-.. ipython:: python
+.. code:: python
 
    dft['A'].dtype
 
@@ -1960,7 +1960,7 @@ If a pandas object contains data with multiple dtypes *in a single column*, the
 dtype of the column will be chosen to accommodate all of the data types
 (``object`` is the most general).
 
-.. ipython:: python
+.. code:: python
 
    # these ints are coerced to floats
    pd.Series([1, 2, 3, 4, 5, 6.])
@@ -1971,7 +1971,7 @@ dtype of the column will be chosen to accommodate all of the data types
 The number of columns of each type in a ``DataFrame`` can be found by calling
 ``DataFrame.dtypes.value_counts()``.
 
-.. ipython:: python
+.. code:: python
 
    dft.dtypes.value_counts()
 
@@ -1980,7 +1980,7 @@ If a dtype is passed (either directly via the ``dtype`` keyword, a passed ``ndar
 or a passed ``Series``, then it will be preserved in DataFrame operations. Furthermore,
 different numeric dtypes will **NOT** be combined. The following example will give you a taste.
 
-.. ipython:: python
+.. code:: python
 
    df1 = pd.DataFrame(np.random.randn(8, 1), columns=['A'], dtype='float32')
    df1
@@ -1999,7 +1999,7 @@ By default integer types are ``int64`` and float types are ``float64``,
 *regardless* of platform (32-bit or 64-bit).
 The following will all result in ``int64`` dtypes.
 
-.. ipython:: python
+.. code:: python
 
    pd.DataFrame([1, 2], columns=['a']).dtypes
    pd.DataFrame({'a': [1, 2]}).dtypes
@@ -2008,7 +2008,7 @@ The following will all result in ``int64`` dtypes.
 Note that Numpy will choose *platform-dependent* types when creating arrays.
 The following **WILL** result in ``int32`` on 32-bit platform.
 
-.. ipython:: python
+.. code:: python
 
    frame = pd.DataFrame(np.array([1, 2]))
 
@@ -2019,7 +2019,7 @@ upcasting
 Types can potentially be *upcasted* when combined with other types, meaning they are promoted
 from the current type (e.g. ``int`` to ``float``).
 
-.. ipython:: python
+.. code:: python
 
    df3 = df1.reindex_like(df2).fillna(value=0.0) + df2
    df3
@@ -2029,7 +2029,7 @@ from the current type (e.g. ``int`` to ``float``).
 the dtype that can accommodate **ALL** of the types in the resulting homogeneous dtyped NumPy array. This can
 force some *upcasting*.
 
-.. ipython:: python
+.. code:: python
 
    df3.to_numpy().dtype
 
@@ -2045,7 +2045,7 @@ exception if the astype operation is invalid.
 Upcasting is always according to the **numpy** rules. If two different dtypes are involved in an operation,
 then the more *general* one will be used as the result of the operation.
 
-.. ipython:: python
+.. code:: python
 
    df3
    df3.dtypes
@@ -2056,7 +2056,7 @@ then the more *general* one will be used as the result of the operation.
 
 Convert a subset of columns to a specified type using :meth:`~DataFrame.astype`.
 
-.. ipython:: python
+.. code:: python
 
    dft = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9]})
    dft[['a', 'b']] = dft[['a', 'b']].astype(np.uint8)
@@ -2067,7 +2067,7 @@ Convert a subset of columns to a specified type using :meth:`~DataFrame.astype`.
 
 Convert certain columns to a specific dtype by passing a dict to :meth:`~DataFrame.astype`.
 
-.. ipython:: python
+.. code:: python
 
    dft1 = pd.DataFrame({'a': [1, 0, 1], 'b': [4, 5, 6], 'c': [7, 8, 9]})
    dft1 = dft1.astype({'a': np.bool, 'c': np.float64})
@@ -2080,7 +2080,7 @@ Convert certain columns to a specific dtype by passing a dict to :meth:`~DataFra
 
     :meth:`~DataFrame.loc` tries to fit in what we are assigning to the current dtypes, while ``[]`` will overwrite them taking the dtype from the right hand side. Therefore the following piece of code produces the unintended result.
 
-    .. ipython:: python
+    .. code:: python
 
        dft = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6], 'c': [7, 8, 9]})
        dft.loc[:, ['a', 'b']].astype(np.uint8).dtypes
@@ -2097,7 +2097,7 @@ In cases where the data is already of the correct type, but stored in an ``objec
 :meth:`DataFrame.infer_objects` and :meth:`Series.infer_objects` methods can be used to soft convert
 to the correct type.
 
-  .. ipython:: python
+  .. code:: python
 
      import datetime
      df = pd.DataFrame([[1, 2],
@@ -2111,7 +2111,7 @@ to the correct type.
 Because the data was transposed the original inference stored all columns as object, which
 ``infer_objects`` will correct.
 
-  .. ipython:: python
+  .. code:: python
 
      df.infer_objects().dtypes
 
@@ -2120,14 +2120,14 @@ hard conversion of objects to a specified type:
 
 * :meth:`~pandas.to_numeric` (conversion to numeric dtypes)
 
-  .. ipython:: python
+  .. code:: python
 
      m = ['1.1', 2, 3]
      pd.to_numeric(m)
 
 * :meth:`~pandas.to_datetime` (conversion to datetime objects)
 
-  .. ipython:: python
+  .. code:: python
 
      import datetime
      m = ['2016-07-09', datetime.datetime(2016, 3, 2)]
@@ -2135,7 +2135,7 @@ hard conversion of objects to a specified type:
 
 * :meth:`~pandas.to_timedelta` (conversion to timedelta objects)
 
-  .. ipython:: python
+  .. code:: python
 
      m = ['5us', pd.Timedelta('1day')]
      pd.to_timedelta(m)
@@ -2147,7 +2147,7 @@ will convert problematic elements to ``pd.NaT`` (for datetime and timedelta) or
 useful if you are reading in data which is mostly of the desired dtype (e.g. numeric, datetime), but occasionally has
 non-conforming elements intermixed that you want to represent as missing:
 
-.. ipython:: python
+.. code:: python
 
     import datetime
     m = ['apple', datetime.datetime(2016, 3, 2)]
@@ -2162,7 +2162,7 @@ non-conforming elements intermixed that you want to represent as missing:
 The ``errors`` parameter has a third option of ``errors='ignore'``, which will simply return the passed in data if it
 encounters any errors with the conversion to a desired data type:
 
-.. ipython:: python
+.. code:: python
 
     import datetime
     m = ['apple', datetime.datetime(2016, 3, 2)]
@@ -2177,7 +2177,7 @@ encounters any errors with the conversion to a desired data type:
 In addition to object conversion, :meth:`~pandas.to_numeric` provides another argument ``downcast``, which gives the
 option of downcasting the newly (or already) numeric data to a smaller dtype, which can conserve memory:
 
-.. ipython:: python
+.. code:: python
 
     m = ['1', 2, 3]
     pd.to_numeric(m, downcast='integer')   # smallest signed int dtype
@@ -2188,7 +2188,7 @@ option of downcasting the newly (or already) numeric data to a smaller dtype, wh
 As these methods apply only to one-dimensional arrays, lists or scalars; they cannot be used directly on multi-dimensional objects such
 as DataFrames. However, with :meth:`~pandas.DataFrame.apply`, we can "apply" the function over each column efficiently:
 
-.. ipython:: python
+.. code:: python
 
     import datetime
     df = pd.DataFrame([
@@ -2211,7 +2211,7 @@ Performing selection operations on ``integer`` type data can easily upcast the d
 The dtype of the input data will be preserved in cases where ``nans`` are not introduced.
 See also :ref:`Support for integer NA <gotchas.intna>`.
 
-.. ipython:: python
+.. code:: python
 
    dfi = df3.astype('int32')
    dfi['E'] = 1
@@ -2224,7 +2224,7 @@ See also :ref:`Support for integer NA <gotchas.intna>`.
 
 While float dtypes are unchanged.
 
-.. ipython:: python
+.. code:: python
 
    dfa = df3.copy()
    dfa['A'] = dfa['A'].astype('float32')
@@ -2245,7 +2245,7 @@ based on their ``dtype``.
 First, let's create a :class:`DataFrame` with a slew of different
 dtypes:
 
-.. ipython:: python
+.. code:: python
 
    df = pd.DataFrame({'string': list('abc'),
                       'int64': list(range(1, 4)),
@@ -2263,7 +2263,7 @@ dtypes:
 
 And the dtypes:
 
-.. ipython:: python
+.. code:: python
 
    df.dtypes
 
@@ -2273,14 +2273,14 @@ columns *without* these dtypes" (``exclude``).
 
 For example, to select ``bool`` columns:
 
-.. ipython:: python
+.. code:: python
 
    df.select_dtypes(include=[bool])
 
 You can also pass the name of a dtype in the `NumPy dtype hierarchy
 <https://docs.scipy.org/doc/numpy/reference/arrays.scalars.html>`__:
 
-.. ipython:: python
+.. code:: python
 
    df.select_dtypes(include=['bool'])
 
@@ -2289,20 +2289,20 @@ You can also pass the name of a dtype in the `NumPy dtype hierarchy
 For example, to select all numeric and boolean columns while excluding unsigned
 integers:
 
-.. ipython:: python
+.. code:: python
 
    df.select_dtypes(include=['number', 'bool'], exclude=['unsignedinteger'])
 
 To select string columns you must use the ``object`` dtype:
 
-.. ipython:: python
+.. code:: python
 
    df.select_dtypes(include=['object'])
 
 To see all the child dtypes of a generic ``dtype`` like ``numpy.number`` you
 can define a function that returns a tree of child dtypes:
 
-.. ipython:: python
+.. code:: python
 
    def subdtypes(dtype):
        subs = dtype.__subclasses__()
@@ -2312,7 +2312,7 @@ can define a function that returns a tree of child dtypes:
 
 All NumPy dtypes are subclasses of ``numpy.generic``:
 
-.. ipython:: python
+.. code:: python
 
     subdtypes(np.generic)
 
diff --git a/doc/source/getting_started/comparison/comparison_with_r.rst b/doc/source/getting_started/comparison/comparison_with_r.rst
index 444e886bc..7e371b7c2 100644
--- a/doc/source/getting_started/comparison/comparison_with_r.rst
+++ b/doc/source/getting_started/comparison/comparison_with_r.rst
@@ -120,7 +120,7 @@ or by integer location
 
 Selecting multiple columns by name in ``pandas`` is straightforward
 
-.. ipython:: python
+.. code:: python
 
    df = pd.DataFrame(np.random.randn(10, 3), columns=list('abc'))
    df[['a', 'c']]
@@ -129,7 +129,7 @@ Selecting multiple columns by name in ``pandas`` is straightforward
 Selecting multiple noncontiguous columns by integer location can be achieved
 with a combination of the ``iloc`` indexer attribute and ``numpy.r_``.
 
-.. ipython:: python
+.. code:: python
 
    named = list('abcdefg')
    n = 30
@@ -157,7 +157,7 @@ Using a data.frame called ``df`` and splitting it into groups ``by1`` and
 The :meth:`~pandas.DataFrame.groupby` method is similar to base R ``aggregate``
 function.
 
-.. ipython:: python
+.. code:: python
 
    df = pd.DataFrame(
        {'v1': [1, 3, 5, 7, 8, 3, 5, np.nan, 4, 5, 7, 9],
@@ -186,7 +186,7 @@ indicating if there is a match or not:
 
 The :meth:`~pandas.DataFrame.isin` method is similar to R ``%in%`` operator:
 
-.. ipython:: python
+.. code:: python
 
    s = pd.Series(np.arange(5), dtype=np.float32)
    s.isin([2, 4])
@@ -222,7 +222,7 @@ since the subclass sizes are possibly irregular. Using a data.frame called
 
 In ``pandas`` we may use :meth:`~pandas.pivot_table` method to handle this:
 
-.. ipython:: python
+.. code:: python
 
    import random
    import string
@@ -254,7 +254,7 @@ In ``pandas``, there are a few ways to perform subsetting. You can use
 :meth:`~pandas.DataFrame.query` or pass an expression as if it were an
 index/slice as well as standard boolean indexing:
 
-.. ipython:: python
+.. code:: python
 
    df = pd.DataFrame({'a': np.random.randn(10), 'b': np.random.randn(10)})
    df.query('a <= b')
@@ -280,7 +280,7 @@ An expression using a data.frame called ``df`` in R with the columns ``a`` and
 In ``pandas`` the equivalent expression, using the
 :meth:`~pandas.DataFrame.eval` method, would be:
 
-.. ipython:: python
+.. code:: python
 
    df = pd.DataFrame({'a': np.random.randn(10), 'b': np.random.randn(10)})
    df.eval('a + b')
@@ -332,7 +332,7 @@ summarize ``x`` by ``month``:
 In ``pandas`` the equivalent expression, using the
 :meth:`~pandas.DataFrame.groupby` method, would be:
 
-.. ipython:: python
+.. code:: python
 
    df = pd.DataFrame({'x': np.random.uniform(1., 168., 120),
                       'y': np.random.uniform(7., 334., 120),
@@ -363,7 +363,7 @@ melt it into a data.frame:
 
 In Python, since ``a`` is a list, you can simply use list comprehension.
 
-.. ipython:: python
+.. code:: python
 
    a = np.array(list(range(1, 24)) + [np.NAN]).reshape(2, 3, 4)
    pd.DataFrame([tuple(list(x) + [val]) for x, val in np.ndenumerate(a)])
@@ -382,7 +382,7 @@ into a data.frame:
 In Python, this list would be a list of tuples, so
 :meth:`~pandas.DataFrame` method would convert it to a dataframe as required.
 
-.. ipython:: python
+.. code:: python
 
    a = list(enumerate(list(range(1, 5)) + [np.NAN]))
    pd.DataFrame(a)
@@ -408,7 +408,7 @@ reshape the data.frame:
 
 In Python, the :meth:`~pandas.melt` method is the R equivalent:
 
-.. ipython:: python
+.. code:: python
 
    cheese = pd.DataFrame({'first': ['John', 'Mary'],
                           'last': ['Doe', 'Bo'],
@@ -442,7 +442,7 @@ into a higher dimensional array:
 
 In Python the best way is to make use of :meth:`~pandas.pivot_table`:
 
-.. ipython:: python
+.. code:: python
 
    df = pd.DataFrame({'x': np.random.uniform(1., 168., 12),
                       'y': np.random.uniform(7., 334., 12),
@@ -473,7 +473,7 @@ aggregate information based on ``Animal`` and ``FeedType``:
 Python can approach this in two different ways. Firstly, similar to above
 using :meth:`~pandas.pivot_table`:
 
-.. ipython:: python
+.. code:: python
 
    df = pd.DataFrame({
        'Animal': ['Animal1', 'Animal2', 'Animal3', 'Animal2', 'Animal1',
@@ -487,7 +487,7 @@ using :meth:`~pandas.pivot_table`:
 
 The second approach is to use the :meth:`~pandas.DataFrame.groupby` method:
 
-.. ipython:: python
+.. code:: python
 
    df.groupby(['Animal', 'FeedType'])['Amount'].sum()
 
@@ -506,7 +506,7 @@ pandas has a data type for categorical data.
 
 In pandas this is accomplished with ``pd.cut`` and ``astype("category")``:
 
-.. ipython:: python
+.. code:: python
 
    pd.cut(pd.Series([1, 2, 3, 4, 5, 6]), 3)
    pd.Series([1, 2, 3, 2, 2, 3]).astype("category")
diff --git a/doc/source/getting_started/comparison/comparison_with_sas.rst b/doc/source/getting_started/comparison/comparison_with_sas.rst
index 69bb700c9..e7fe044e4 100644
--- a/doc/source/getting_started/comparison/comparison_with_sas.rst
+++ b/doc/source/getting_started/comparison/comparison_with_sas.rst
@@ -13,7 +13,7 @@ to familiarize yourself with the library.
 
 As is customary, we import pandas and NumPy as follows:
 
-.. ipython:: python
+.. code:: python
 
     import pandas as pd
     import numpy as np
@@ -104,7 +104,7 @@ but for a small number of values, it is often convenient to specify it as
 a Python dictionary, where the keys are the column names
 and the values are the data.
 
-.. ipython:: python
+.. code:: python
 
    df = pd.DataFrame({'x': [1, 3, 5], 'y': [2, 4, 6]})
    df
@@ -128,7 +128,7 @@ SAS provides ``PROC IMPORT`` to read csv data into a data set.
 
 The pandas method is :func:`read_csv`, which works similarly.
 
-.. ipython:: python
+.. code:: python
 
    url = ('https://raw.github.com/pandas-dev/'
           'pandas/master/pandas/tests/data/tips.csv')
@@ -190,13 +190,13 @@ pandas provides similar vectorized operations by
 specifying the individual ``Series`` in the ``DataFrame``.
 New columns can be assigned in the same way.
 
-.. ipython:: python
+.. code:: python
 
    tips['total_bill'] = tips['total_bill'] - 2
    tips['new_bill'] = tips['total_bill'] / 2.0
    tips.head()
 
-.. ipython:: python
+.. code:: python
    :suppress:
 
    tips = tips.drop('new_bill', axis=1)
@@ -224,7 +224,7 @@ or more columns.
 DataFrames can be filtered in multiple ways; the most intuitive of which is using
 :ref:`boolean indexing <indexing.boolean>`
 
-.. ipython:: python
+.. code:: python
 
    tips[tips['total_bill'] > 10].head()
 
@@ -246,12 +246,12 @@ In SAS, if/then logic can be used to create new columns.
 The same operation in pandas can be accomplished using
 the ``where`` method from ``numpy``.
 
-.. ipython:: python
+.. code:: python
 
    tips['bucket'] = np.where(tips['total_bill'] < 10, 'low', 'high')
    tips.head()
 
-.. ipython:: python
+.. code:: python
    :suppress:
 
    tips = tips.drop('bucket', axis=1)
@@ -282,7 +282,7 @@ functions pandas supports other Time Series features
 not available in Base SAS (such as resampling and custom offsets) -
 see the :ref:`timeseries documentation<timeseries>` for more details.
 
-.. ipython:: python
+.. code:: python
 
    tips['date1'] = pd.Timestamp('2013-01-15')
    tips['date2'] = pd.Timestamp('2015-02-15')
@@ -295,7 +295,7 @@ see the :ref:`timeseries documentation<timeseries>` for more details.
    tips[['date1', 'date2', 'date1_year', 'date2_month',
          'date1_next', 'months_between']].head()
 
-.. ipython:: python
+.. code:: python
    :suppress:
 
    tips = tips.drop(['date1', 'date2', 'date1_year',
@@ -326,7 +326,7 @@ drop, and rename columns.
 
 The same operations are expressed in pandas below.
 
-.. ipython:: python
+.. code:: python
 
    # keep
    tips[['sex', 'total_bill', 'tip']].head()
@@ -352,7 +352,7 @@ Sorting in SAS is accomplished via ``PROC SORT``
 pandas objects have a :meth:`~DataFrame.sort_values` method, which
 takes a list of columns to sort by.
 
-.. ipython:: python
+.. code:: python
 
    tips = tips.sort_values(['sex', 'total_bill'])
    tips.head()
@@ -381,7 +381,7 @@ Python determines the length of a character string with the ``len`` function.
 ``len`` includes trailing blanks.  Use ``len`` and ``rstrip`` to exclude
 trailing blanks.
 
-.. ipython:: python
+.. code:: python
 
    tips['time'].str.len().head()
    tips['time'].str.rstrip().str.len().head()
@@ -408,7 +408,7 @@ substring.  If the substring is found, the function returns its
 position.  Keep in mind that Python indexes are zero-based and
 the function will return -1 if it fails to find the substring.
 
-.. ipython:: python
+.. code:: python
 
    tips['sex'].str.find("ale").head()
 
@@ -430,7 +430,7 @@ With pandas you can use ``[]`` notation to extract a substring
 from a string by position locations.  Keep in mind that Python
 indexes are zero-based.
 
-.. ipython:: python
+.. code:: python
 
    tips['sex'].str[0:1].head()
 
@@ -458,7 +458,7 @@ Python extracts a substring from a string based on its text
 by using regular expressions. There are much more powerful
 approaches, but this just shows a simple approach.
 
-.. ipython:: python
+.. code:: python
 
    firstlast = pd.DataFrame({'String': ['John Smith', 'Jane Cook']})
    firstlast['First_Name'] = firstlast['String'].str.split(" ", expand=True)[0]
@@ -489,7 +489,7 @@ functions change the case of the argument.
 
 The equivalent Python functions are ``upper``, ``lower``, and ``title``.
 
-.. ipython:: python
+.. code:: python
 
    firstlast = pd.DataFrame({'String': ['John Smith', 'Jane Cook']})
    firstlast['string_up'] = firstlast['String'].str.upper()
@@ -502,7 +502,7 @@ Merging
 
 The following tables will be used in the merge examples
 
-.. ipython:: python
+.. code:: python
 
    df1 = pd.DataFrame({'key': ['A', 'B', 'C', 'D'],
                        'value': np.random.randn(4)})
@@ -540,7 +540,7 @@ similar functionality.  Note that the data does not have
 to be sorted ahead of time, and different join
 types are accomplished via the ``how`` keyword.
 
-.. ipython:: python
+.. code:: python
 
    inner_join = df1.merge(df2, on=['key'], how='inner')
    inner_join
@@ -563,7 +563,7 @@ special float value ``NaN`` (not a number).  Many of the semantics
 are the same, for example missing data propagates through numeric
 operations, and is ignored by default for aggregations.
 
-.. ipython:: python
+.. code:: python
 
    outer_join
    outer_join['value_x'] + outer_join['value_y']
@@ -587,7 +587,7 @@ For example, in SAS you could do this to filter missing values.
 Which doesn't work in pandas.  Instead, the ``pd.isna`` or ``pd.notna`` functions
 should be used for comparisons.
 
-.. ipython:: python
+.. code:: python
 
    outer_join[pd.isna(outer_join['value_x'])]
    outer_join[pd.notna(outer_join['value_x'])]
@@ -598,7 +598,7 @@ drop all rows with any missing values, replacing missing values with a specified
 value, like the mean, or forward filling from previous rows. See the
 :ref:`missing data documentation<missing_data>` for more.
 
-.. ipython:: python
+.. code:: python
 
    outer_join.dropna()
    outer_join.fillna(method='ffill')
@@ -627,7 +627,7 @@ pandas provides a flexible ``groupby`` mechanism that
 allows similar aggregations.  See the :ref:`groupby documentation<groupby>`
 for more details and examples.
 
-.. ipython:: python
+.. code:: python
 
    tips_summed = tips.groupby(['sex', 'smoker'])['total_bill', 'tip'].sum()
    tips_summed.head()
@@ -664,7 +664,7 @@ pandas ``groupby`` provides a ``transform`` mechanism that allows
 these type of operations to be succinctly expressed in one
 operation.
 
-.. ipython:: python
+.. code:: python
 
    gb = tips.groupby('smoker')['total_bill']
    tips['adj_total_bill'] = tips['total_bill'] - gb.transform('mean')
@@ -693,7 +693,7 @@ the first entry for each.
 
 In pandas this would be written as:
 
-.. ipython:: python
+.. code:: python
 
    tips.groupby(['sex', 'smoker']).first()
 
diff --git a/doc/source/getting_started/comparison/comparison_with_sql.rst b/doc/source/getting_started/comparison/comparison_with_sql.rst
index 366fdd546..fb161aff0 100644
--- a/doc/source/getting_started/comparison/comparison_with_sql.rst
+++ b/doc/source/getting_started/comparison/comparison_with_sql.rst
@@ -13,7 +13,7 @@ to familiarize yourself with the library.
 
 As is customary, we import pandas and NumPy as follows:
 
-.. ipython:: python
+.. code:: python
 
     import pandas as pd
     import numpy as np
@@ -22,7 +22,7 @@ Most of the examples will utilize the ``tips`` dataset found within pandas tests
 the data into a DataFrame called `tips` and assume we have a database table of the same name and
 structure.
 
-.. ipython:: python
+.. code:: python
 
     url = ('https://raw.github.com/pandas-dev'
            '/pandas/master/pandas/tests/data/tips.csv')
@@ -42,7 +42,7 @@ to select all columns):
 
 With pandas, column selection is done by passing a list of column names to your DataFrame:
 
-.. ipython:: python
+.. code:: python
 
     tips[['total_bill', 'tip', 'smoker', 'time']].head(5)
 
@@ -63,14 +63,14 @@ Filtering in SQL is done via a WHERE clause.
 DataFrames can be filtered in multiple ways; the most intuitive of which is using
 `boolean indexing <https://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing>`_.
 
-.. ipython:: python
+.. code:: python
 
     tips[tips['time'] == 'Dinner'].head(5)
 
 The above statement is simply passing a ``Series`` of True/False objects to the DataFrame,
 returning all rows with True.
 
-.. ipython:: python
+.. code:: python
 
     is_dinner = tips['time'] == 'Dinner'
     is_dinner.value_counts()
@@ -86,7 +86,7 @@ Just like SQL's OR and AND, multiple conditions can be passed to a DataFrame usi
     FROM tips
     WHERE time = 'Dinner' AND tip > 5.00;
 
-.. ipython:: python
+.. code:: python
 
     # tips of more than $5.00 at Dinner meals
     tips[(tips['time'] == 'Dinner') & (tips['tip'] > 5.00)]
@@ -98,7 +98,7 @@ Just like SQL's OR and AND, multiple conditions can be passed to a DataFrame usi
     FROM tips
     WHERE size >= 5 OR total_bill > 45;
 
-.. ipython:: python
+.. code:: python
 
     # tips by parties of at least 5 diners OR bill total was more than $45
     tips[(tips['size'] >= 5) | (tips['total_bill'] > 45)]
@@ -106,7 +106,7 @@ Just like SQL's OR and AND, multiple conditions can be passed to a DataFrame usi
 NULL checking is done using the :meth:`~pandas.Series.notna` and :meth:`~pandas.Series.isna`
 methods.
 
-.. ipython:: python
+.. code:: python
 
     frame = pd.DataFrame({'col1': ['A', 'B', np.NaN, 'C', 'D'],
                           'col2': ['F', np.NaN, 'G', 'H', 'I']})
@@ -121,7 +121,7 @@ where ``col2`` IS NULL with the following query:
     FROM frame
     WHERE col2 IS NULL;
 
-.. ipython:: python
+.. code:: python
 
     frame[frame['col2'].isna()]
 
@@ -133,7 +133,7 @@ Getting items where ``col1`` IS NOT NULL can be done with :meth:`~pandas.Series.
     FROM frame
     WHERE col1 IS NOT NULL;
 
-.. ipython:: python
+.. code:: python
 
     frame[frame['col1'].notna()]
 
@@ -161,7 +161,7 @@ For instance, a query getting us the number of tips left by sex:
 
 The pandas equivalent would be:
 
-.. ipython:: python
+.. code:: python
 
     tips.groupby('sex').size()
 
@@ -170,14 +170,14 @@ Notice that in the pandas code we used :meth:`~pandas.core.groupby.DataFrameGrou
 :meth:`~pandas.core.groupby.DataFrameGroupBy.count` applies the function to each column, returning
 the number of ``not null`` records within each.
 
-.. ipython:: python
+.. code:: python
 
     tips.groupby('sex').count()
 
 Alternatively, we could have applied the :meth:`~pandas.core.groupby.DataFrameGroupBy.count` method
 to an individual column:
 
-.. ipython:: python
+.. code:: python
 
     tips.groupby('sex')['total_bill'].count()
 
@@ -197,7 +197,7 @@ to your grouped DataFrame, indicating which functions to apply to specific colum
     Thur  2.771452   62
     */
 
-.. ipython:: python
+.. code:: python
 
     tips.groupby('day').agg({'tip': np.mean, 'day': np.size})
 
@@ -221,7 +221,7 @@ Grouping by more than one column is done by passing a list of columns to the
            Thur    17  3.030000
     */
 
-.. ipython:: python
+.. code:: python
 
     tips.groupby(['smoker', 'day']).agg({'tip': [np.size, np.mean]})
 
@@ -234,7 +234,7 @@ JOINs can be performed with :meth:`~pandas.DataFrame.join` or :meth:`~pandas.mer
 parameters allowing you to specify the type of join to perform (LEFT, RIGHT, INNER, FULL) or the
 columns to join on (column names or indices).
 
-.. ipython:: python
+.. code:: python
 
     df1 = pd.DataFrame({'key': ['A', 'B', 'C', 'D'],
                         'value': np.random.randn(4)})
@@ -254,7 +254,7 @@ INNER JOIN
     INNER JOIN df2
       ON df1.key = df2.key;
 
-.. ipython:: python
+.. code:: python
 
     # merge performs an INNER JOIN by default
     pd.merge(df1, df2, on='key')
@@ -262,7 +262,7 @@ INNER JOIN
 :meth:`~pandas.merge` also offers parameters for cases when you'd like to join one DataFrame's
 column with another DataFrame's index.
 
-.. ipython:: python
+.. code:: python
 
     indexed_df2 = df2.set_index('key')
     pd.merge(df1, indexed_df2, left_on='key', right_index=True)
@@ -277,7 +277,7 @@ LEFT OUTER JOIN
     LEFT OUTER JOIN df2
       ON df1.key = df2.key;
 
-.. ipython:: python
+.. code:: python
 
     # show all records from df1
     pd.merge(df1, df2, on='key', how='left')
@@ -292,7 +292,7 @@ RIGHT JOIN
     RIGHT OUTER JOIN df2
       ON df1.key = df2.key;
 
-.. ipython:: python
+.. code:: python
 
     # show all records from df2
     pd.merge(df1, df2, on='key', how='right')
@@ -310,7 +310,7 @@ joined columns find a match. As of writing, FULL JOINs are not supported in all
     FULL OUTER JOIN df2
       ON df1.key = df2.key;
 
-.. ipython:: python
+.. code:: python
 
     # show all records from both frames
     pd.merge(df1, df2, on='key', how='outer')
@@ -320,7 +320,7 @@ UNION
 -----
 UNION ALL can be performed using :meth:`~pandas.concat`.
 
-.. ipython:: python
+.. code:: python
 
     df1 = pd.DataFrame({'city': ['Chicago', 'San Francisco', 'New York City'],
                         'rank': range(1, 4)})
@@ -344,7 +344,7 @@ UNION ALL can be performed using :meth:`~pandas.concat`.
       Los Angeles     5
     */
 
-.. ipython:: python
+.. code:: python
 
     pd.concat([df1, df2])
 
@@ -370,7 +370,7 @@ SQL's UNION is similar to UNION ALL, however UNION will remove duplicate rows.
 In pandas, you can use :meth:`~pandas.concat` in conjunction with
 :meth:`~pandas.DataFrame.drop_duplicates`.
 
-.. ipython:: python
+.. code:: python
 
     pd.concat([df1, df2]).drop_duplicates()
 
@@ -387,7 +387,7 @@ Top N rows with offset
     ORDER BY tip DESC
     LIMIT 10 OFFSET 5;
 
-.. ipython:: python
+.. code:: python
 
     tips.nlargest(10 + 5, columns='tip').tail(10)
 
@@ -407,7 +407,7 @@ Top N rows per group
     ORDER BY day, rn;
 
 
-.. ipython:: python
+.. code:: python
 
     (tips.assign(rn=tips.sort_values(['total_bill'], ascending=False)
                         .groupby(['day'])
@@ -417,7 +417,7 @@ Top N rows per group
 
 the same using `rank(method='first')` function
 
-.. ipython:: python
+.. code:: python
 
     (tips.assign(rnk=tips.groupby(['day'])['total_bill']
                          .rank(method='first', ascending=False))
@@ -442,7 +442,7 @@ Notice that when using ``rank(method='min')`` function
 `rnk_min` remains the same for the same `tip`
 (as Oracle's RANK() function)
 
-.. ipython:: python
+.. code:: python
 
     (tips[tips['tip'] < 2]
         .assign(rnk_min=tips.groupby(['sex'])['tip']
@@ -460,7 +460,7 @@ UPDATE
     SET tip = tip*2
     WHERE tip < 2;
 
-.. ipython:: python
+.. code:: python
 
     tips.loc[tips['tip'] < 2, 'tip'] *= 2
 
@@ -474,6 +474,6 @@ DELETE
 
 In pandas we select the rows that should remain, instead of deleting them
 
-.. ipython:: python
+.. code:: python
 
     tips = tips.loc[tips['tip'] <= 9]
diff --git a/doc/source/getting_started/comparison/comparison_with_stata.rst b/doc/source/getting_started/comparison/comparison_with_stata.rst
index db6873863..beb25ad1e 100644
--- a/doc/source/getting_started/comparison/comparison_with_stata.rst
+++ b/doc/source/getting_started/comparison/comparison_with_stata.rst
@@ -14,7 +14,7 @@ to familiarize yourself with the library.
 As is customary, we import pandas and NumPy as follows. This means that we can refer to the
 libraries as ``pd`` and ``np``, respectively, for the rest of the document.
 
-.. ipython:: python
+.. code:: python
 
     import pandas as pd
     import numpy as np
@@ -101,7 +101,7 @@ but for a small number of values, it is often convenient to specify it as
 a Python dictionary, where the keys are the column names
 and the values are the data.
 
-.. ipython:: python
+.. code:: python
 
    df = pd.DataFrame({'x': [1, 3, 5], 'y': [2, 4, 6]})
    df
@@ -125,7 +125,7 @@ If the ``tips.csv`` file is in the current working directory, we can import it a
 The pandas method is :func:`read_csv`, which works similarly. Additionally, it will automatically download
 the data set if presented with a url.
 
-.. ipython:: python
+.. code:: python
 
    url = ('https://raw.github.com/pandas-dev'
           '/pandas/master/pandas/tests/data/tips.csv')
@@ -198,7 +198,7 @@ specifying the individual ``Series`` in the ``DataFrame``.
 New columns can be assigned in the same way. The :meth:`DataFrame.drop` method
 drops a column from the ``DataFrame``.
 
-.. ipython:: python
+.. code:: python
 
    tips['total_bill'] = tips['total_bill'] - 2
    tips['new_bill'] = tips['total_bill'] / 2
@@ -218,7 +218,7 @@ Filtering in Stata is done with an ``if`` clause on one or more columns.
 DataFrames can be filtered in multiple ways; the most intuitive of which is using
 :ref:`boolean indexing <indexing.boolean>`.
 
-.. ipython:: python
+.. code:: python
 
    tips[tips['total_bill'] > 10].head()
 
@@ -235,12 +235,12 @@ In Stata, an ``if`` clause can also be used to create new columns.
 The same operation in pandas can be accomplished using
 the ``where`` method from ``numpy``.
 
-.. ipython:: python
+.. code:: python
 
    tips['bucket'] = np.where(tips['total_bill'] < 10, 'low', 'high')
    tips.head()
 
-.. ipython:: python
+.. code:: python
    :suppress:
 
    tips = tips.drop('bucket', axis=1)
@@ -271,7 +271,7 @@ functions, pandas supports other Time Series features
 not available in Stata (such as time zone handling and custom offsets) --
 see the :ref:`timeseries documentation<timeseries>` for more details.
 
-.. ipython:: python
+.. code:: python
 
    tips['date1'] = pd.Timestamp('2013-01-15')
    tips['date2'] = pd.Timestamp('2015-02-15')
@@ -284,7 +284,7 @@ see the :ref:`timeseries documentation<timeseries>` for more details.
    tips[['date1', 'date2', 'date1_year', 'date2_month', 'date1_next',
          'months_between']].head()
 
-.. ipython:: python
+.. code:: python
    :suppress:
 
    tips = tips.drop(['date1', 'date2', 'date1_year', 'date2_month',
@@ -307,7 +307,7 @@ The same operations are expressed in pandas below. Note that in contrast to Stat
 operations do not happen in place. To make these changes persist, assign the operation back
 to a variable.
 
-.. ipython:: python
+.. code:: python
 
    # keep
    tips[['sex', 'total_bill', 'tip']].head()
@@ -331,7 +331,7 @@ Sorting in Stata is accomplished via ``sort``
 pandas objects have a :meth:`DataFrame.sort_values` method, which
 takes a list of columns to sort by.
 
-.. ipython:: python
+.. code:: python
 
    tips = tips.sort_values(['sex', 'total_bill'])
    tips.head()
@@ -355,7 +355,7 @@ Python determines the length of a character string with the ``len`` function.
 In Python 3, all strings are Unicode strings. ``len`` includes trailing blanks.
 Use ``len`` and ``rstrip`` to exclude trailing blanks.
 
-.. ipython:: python
+.. code:: python
 
    tips['time'].str.len().head()
    tips['time'].str.rstrip().str.len().head()
@@ -378,7 +378,7 @@ substring.  If the substring is found, the function returns its
 position.  Keep in mind that Python indexes are zero-based and
 the function will return -1 if it fails to find the substring.
 
-.. ipython:: python
+.. code:: python
 
    tips['sex'].str.find("ale").head()
 
@@ -396,7 +396,7 @@ With pandas you can use ``[]`` notation to extract a substring
 from a string by position locations.  Keep in mind that Python
 indexes are zero-based.
 
-.. ipython:: python
+.. code:: python
 
    tips['sex'].str[0:1].head()
 
@@ -423,7 +423,7 @@ Python extracts a substring from a string based on its text
 by using regular expressions. There are much more powerful
 approaches, but this just shows a simple approach.
 
-.. ipython:: python
+.. code:: python
 
    firstlast = pd.DataFrame({'string': ['John Smith', 'Jane Cook']})
    firstlast['First_Name'] = firstlast['string'].str.split(" ", expand=True)[0]
@@ -453,7 +453,7 @@ change the case of ASCII and Unicode strings, respectively.
 
 The equivalent Python functions are ``upper``, ``lower``, and ``title``.
 
-.. ipython:: python
+.. code:: python
 
    firstlast = pd.DataFrame({'string': ['John Smith', 'Jane Cook']})
    firstlast['upper'] = firstlast['string'].str.upper()
@@ -466,7 +466,7 @@ Merging
 
 The following tables will be used in the merge examples
 
-.. ipython:: python
+.. code:: python
 
    df1 = pd.DataFrame({'key': ['A', 'B', 'C', 'D'],
                        'value': np.random.randn(4)})
@@ -532,7 +532,7 @@ pandas DataFrames have a :meth:`DataFrame.merge` method, which provides
 similar functionality. Note that different join
 types are accomplished via the ``how`` keyword.
 
-.. ipython:: python
+.. code:: python
 
    inner_join = df1.merge(df2, on=['key'], how='inner')
    inner_join
@@ -555,7 +555,7 @@ special float value ``NaN`` (not a number).  Many of the semantics
 are the same; for example missing data propagates through numeric
 operations, and is ignored by default for aggregations.
 
-.. ipython:: python
+.. code:: python
 
    outer_join
    outer_join['value_x'] + outer_join['value_y']
@@ -574,7 +574,7 @@ For example, in Stata you could do this to filter missing values.
 This doesn't work in pandas.  Instead, the :func:`pd.isna` or :func:`pd.notna` functions
 should be used for comparisons.
 
-.. ipython:: python
+.. code:: python
 
    outer_join[pd.isna(outer_join['value_x'])]
    outer_join[pd.notna(outer_join['value_x'])]
@@ -585,7 +585,7 @@ drop all rows with any missing values, replacing missing values with a specified
 value, like the mean, or forward filling from previous rows. See the
 :ref:`missing data documentation<missing_data>` for more.
 
-.. ipython:: python
+.. code:: python
 
    # Drop rows with any missing value
    outer_join.dropna()
@@ -615,7 +615,7 @@ pandas provides a flexible ``groupby`` mechanism that
 allows similar aggregations.  See the :ref:`groupby documentation<groupby>`
 for more details and examples.
 
-.. ipython:: python
+.. code:: python
 
    tips_summed = tips.groupby(['sex', 'smoker'])['total_bill', 'tip'].sum()
    tips_summed.head()
@@ -638,7 +638,7 @@ pandas ``groupby`` provides a ``transform`` mechanism that allows
 these type of operations to be succinctly expressed in one
 operation.
 
-.. ipython:: python
+.. code:: python
 
    gb = tips.groupby('smoker')['total_bill']
    tips['adj_total_bill'] = tips['total_bill'] - gb.transform('mean')
@@ -659,7 +659,7 @@ sort order by sex/smoker group.
 
 In pandas this would be written as:
 
-.. ipython:: python
+.. code:: python
 
    tips.groupby(['sex', 'smoker']).first()
 
diff --git a/doc/source/getting_started/dsintro.rst b/doc/source/getting_started/dsintro.rst
index 2fb0b1636..ca972a0c1 100644
--- a/doc/source/getting_started/dsintro.rst
+++ b/doc/source/getting_started/dsintro.rst
@@ -11,7 +11,7 @@ structures in pandas to get you started. The fundamental behavior about data
 types, indexing, and axis labeling / alignment apply across all of the
 objects. To get started, import NumPy and load pandas into your namespace:
 
-.. ipython:: python
+.. code:: python
 
    import numpy as np
    import pandas as pd
@@ -49,7 +49,7 @@ cases depending on what **data is**:
 If ``data`` is an ndarray, **index** must be the same length as **data**. If no
 index is passed, one will be created having values ``[0, ..., len(data) - 1]``.
 
-.. ipython:: python
+.. code:: python
 
    s = pd.Series(np.random.randn(5), index=['a', 'b', 'c', 'd', 'e'])
    s
@@ -69,7 +69,7 @@ index is passed, one will be created having values ``[0, ..., len(data) - 1]``.
 
 Series can be instantiated from dicts:
 
-.. ipython:: python
+.. code:: python
 
    d = {'b': 1, 'a': 0, 'c': 2}
    pd.Series(d)
@@ -90,7 +90,7 @@ order of the dict keys (i.e. ``['a', 'b', 'c']`` rather than ``['b', 'a', 'c']``
 If an index is passed, the values in data corresponding to the labels in the
 index will be pulled out.
 
-.. ipython:: python
+.. code:: python
 
    d = {'a': 0., 'b': 1., 'c': 2.}
    pd.Series(d)
@@ -105,7 +105,7 @@ index will be pulled out.
 If ``data`` is a scalar value, an index must be
 provided. The value will be repeated to match the length of **index**.
 
-.. ipython:: python
+.. code:: python
 
    pd.Series(5., index=['a', 'b', 'c', 'd', 'e'])
 
@@ -115,7 +115,7 @@ Series is ndarray-like
 ``Series`` acts very similarly to a ``ndarray``, and is a valid argument to most NumPy functions.
 However, operations such as slicing will also slice the index.
 
-.. ipython:: python
+.. code:: python
 
     s[0]
     s[:3]
@@ -130,7 +130,7 @@ However, operations such as slicing will also slice the index.
 
 Like a NumPy array, a pandas Series has a :attr:`~Series.dtype`.
 
-.. ipython:: python
+.. code:: python
 
    s.dtype
 
@@ -142,7 +142,7 @@ for more.
 
 If you need the actual array backing a ``Series``, use :attr:`Series.array`.
 
-.. ipython:: python
+.. code:: python
 
    s.array
 
@@ -158,7 +158,7 @@ See :ref:`basics.dtypes` for more.
 While Series is ndarray-like, if you need an *actual* ndarray, then use
 :meth:`Series.to_numpy`.
 
-.. ipython:: python
+.. code:: python
 
    s.to_numpy()
 
@@ -171,7 +171,7 @@ Series is dict-like
 A Series is like a fixed-size dict in that you can get and set values by index
 label:
 
-.. ipython:: python
+.. code:: python
 
     s['a']
     s['e'] = 12.
@@ -188,7 +188,7 @@ If a label is not contained, an exception is raised:
 
 Using the ``get`` method, a missing label will return None or specified default:
 
-.. ipython:: python
+.. code:: python
 
    s.get('f')
 
@@ -203,7 +203,7 @@ When working with raw NumPy arrays, looping through value-by-value is usually
 not necessary. The same is true when working with Series in pandas.
 Series can also be passed into most NumPy methods expecting an ndarray.
 
-.. ipython:: python
+.. code:: python
 
     s + s
     s * 2
@@ -214,7 +214,7 @@ automatically align the data based on label. Thus, you can write computations
 without giving consideration to whether the Series involved have the same
 labels.
 
-.. ipython:: python
+.. code:: python
 
     s[1:] + s[:-1]
 
@@ -242,7 +242,7 @@ Name attribute
 
 Series can also have a ``name`` attribute:
 
-.. ipython:: python
+.. code:: python
 
    s = pd.Series(np.random.randn(5), name='something')
    s
@@ -255,7 +255,7 @@ when taking 1D slices of DataFrame as you will see below.
 
 You can rename a Series with the :meth:`pandas.Series.rename` method.
 
-.. ipython:: python
+.. code:: python
 
    s2 = s.rename("different")
    s2.name
@@ -306,7 +306,7 @@ Series. If there are any nested dicts, these will first be converted to
 Series. If no columns are passed, the columns will be the ordered list of dict
 keys.
 
-.. ipython:: python
+.. code:: python
 
     d = {'one': pd.Series([1., 2., 3.], index=['a', 'b', 'c']),
          'two': pd.Series([1., 2., 3., 4.], index=['a', 'b', 'c', 'd'])}
@@ -324,7 +324,7 @@ The row and column labels can be accessed respectively by accessing the
    When a particular set of columns is passed along with a dict of data, the
    passed columns override the keys in the dict.
 
-.. ipython:: python
+.. code:: python
 
    df.index
    df.columns
@@ -336,7 +336,7 @@ The ndarrays must all be the same length. If an index is passed, it must
 clearly also be the same length as the arrays. If no index is passed, the
 result will be ``range(n)``, where ``n`` is the array length.
 
-.. ipython:: python
+.. code:: python
 
    d = {'one': [1., 2., 3., 4.],
         'two': [4., 3., 2., 1.]}
@@ -348,7 +348,7 @@ From structured or record array
 
 This case is handled identically to a dict of arrays.
 
-.. ipython:: python
+.. code:: python
 
    data = np.zeros((2, ), dtype=[('A', 'i4'), ('B', 'f4'), ('C', 'a10')])
    data[:] = [(1, 2., 'Hello'), (2, 3., "World")]
@@ -367,7 +367,7 @@ This case is handled identically to a dict of arrays.
 From a list of dicts
 ~~~~~~~~~~~~~~~~~~~~
 
-.. ipython:: python
+.. code:: python
 
    data2 = [{'a': 1, 'b': 2}, {'a': 5, 'b': 10, 'c': 20}]
    pd.DataFrame(data2)
@@ -382,7 +382,7 @@ From a dict of tuples
 You can automatically create a MultiIndexed frame by passing a tuples
 dictionary.
 
-.. ipython:: python
+.. code:: python
 
    pd.DataFrame({('a', 'b'): {('A', 'B'): 1, ('A', 'C'): 2},
                  ('a', 'a'): {('A', 'C'): 3, ('A', 'B'): 4},
@@ -420,14 +420,14 @@ for the ``orient`` parameter which is ``'columns'`` by default, but which can be
 set to ``'index'`` in order to use the dict keys as row labels.
 
 
-.. ipython:: python
+.. code:: python
 
    pd.DataFrame.from_dict(dict([('A', [1, 2, 3]), ('B', [4, 5, 6])]))
 
 If you pass ``orient='index'``, the keys will be the row labels. In this
 case, you can also pass the desired column names:
 
-.. ipython:: python
+.. code:: python
 
    pd.DataFrame.from_dict(dict([('A', [1, 2, 3]), ('B', [4, 5, 6])]),
                           orient='index', columns=['one', 'two', 'three'])
@@ -441,7 +441,7 @@ dtype. It works analogously to the normal ``DataFrame`` constructor, except that
 the resulting DataFrame index may be a specific field of the structured
 dtype. For example:
 
-.. ipython:: python
+.. code:: python
 
    data
    pd.DataFrame.from_records(data, index='C')
@@ -454,7 +454,7 @@ You can treat a DataFrame semantically like a dict of like-indexed Series
 objects. Getting, setting, and deleting columns works with the same syntax as
 the analogous dict operations:
 
-.. ipython:: python
+.. code:: python
 
    df['one']
    df['three'] = df['one'] * df['two']
@@ -463,7 +463,7 @@ the analogous dict operations:
 
 Columns can be deleted or popped like with a dict:
 
-.. ipython:: python
+.. code:: python
 
    del df['two']
    three = df.pop('three')
@@ -472,7 +472,7 @@ Columns can be deleted or popped like with a dict:
 When inserting a scalar value, it will naturally be propagated to fill the
 column:
 
-.. ipython:: python
+.. code:: python
 
    df['foo'] = 'bar'
    df
@@ -480,7 +480,7 @@ column:
 When inserting a Series that does not have the same index as the DataFrame, it
 will be conformed to the DataFrame's index:
 
-.. ipython:: python
+.. code:: python
 
    df['one_trunc'] = df['one'][:2]
    df
@@ -491,7 +491,7 @@ DataFrame's index.
 By default, columns get inserted at the end. The ``insert`` function is
 available to insert at a particular location in the columns:
 
-.. ipython:: python
+.. code:: python
 
    df.insert(1, 'bar', df['one'])
    df
@@ -507,7 +507,7 @@ Inspired by `dplyr's
 method that allows you to easily create new columns that are potentially
 derived from existing columns.
 
-.. ipython:: python
+.. code:: python
 
    iris = pd.read_csv('data/iris.data')
    iris.head()
@@ -517,7 +517,7 @@ derived from existing columns.
 In the example above, we inserted a precomputed value. We can also pass in
 a function of one argument to be evaluated on the DataFrame being assigned to.
 
-.. ipython:: python
+.. code:: python
 
    iris.assign(sepal_ratio=lambda x: (x['SepalWidth'] / x['SepalLength'])).head()
 
@@ -530,9 +530,9 @@ common when using ``assign`` in a chain of operations. For example,
 we can limit the DataFrame to just those observations with a Sepal Length
 greater than 5, calculate the ratio, and plot:
 
-.. ipython:: python
+.. code:: python
+
 
-   @savefig basics_assign.png
    (iris.query('SepalLength > 5')
         .assign(SepalRatio=lambda x: x.SepalWidth / x.SepalLength,
                 PetalRatio=lambda x: x.PetalWidth / x.PetalLength)
@@ -556,7 +556,7 @@ Starting with Python 3.6 the order of ``**kwargs`` is preserved. This allows
 for *dependent* assignment, where an expression later in ``**kwargs`` can refer
 to a column created earlier in the same :meth:`~DataFrame.assign`.
 
-.. ipython:: python
+.. code:: python
 
    dfa = pd.DataFrame({"A": [1, 2, 3],
                        "B": [4, 5, 6]})
@@ -568,7 +568,7 @@ that's equal to ``dfa['A'] + dfa['B']``.
 
 To write code compatible with all versions of Python, split the assignment in two.
 
-.. ipython:: python
+.. code:: python
 
    dependent = pd.DataFrame({"A": [1, 1, 1]})
    (dependent.assign(A=lambda x: x['A'] + 1)
@@ -631,7 +631,7 @@ The basics of indexing are as follows:
 Row selection, for example, returns a Series whose index is the columns of the
 DataFrame:
 
-.. ipython:: python
+.. code:: python
 
    df.loc['b']
    df.iloc[2]
@@ -650,7 +650,7 @@ Data alignment between DataFrame objects automatically align on **both the
 columns and the index (row labels)**. Again, the resulting object will have the
 union of the column and row labels.
 
-.. ipython:: python
+.. code:: python
 
     df = pd.DataFrame(np.random.randn(10, 4), columns=['A', 'B', 'C', 'D'])
     df2 = pd.DataFrame(np.random.randn(7, 3), columns=['A', 'B', 'C'])
@@ -661,14 +661,14 @@ to align the Series **index** on the DataFrame **columns**, thus `broadcasting
 <http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html>`__
 row-wise. For example:
 
-.. ipython:: python
+.. code:: python
 
    df - df.iloc[0]
 
 In the special case of working with time series data, if the DataFrame index
 contains dates, the broadcasting will be column-wise:
 
-.. ipython:: python
+.. code:: python
    :okwarning:
 
    index = pd.date_range('1/1/2000', periods=8)
@@ -695,7 +695,7 @@ section on :ref:`flexible binary operations <basics.binop>`.
 
 Operations with scalars are just as you would expect:
 
-.. ipython:: python
+.. code:: python
 
    df * 5 + 2
    1 / df
@@ -705,7 +705,7 @@ Operations with scalars are just as you would expect:
 
 Boolean operators work as well:
 
-.. ipython:: python
+.. code:: python
 
    df1 = pd.DataFrame({'a': [1, 0, 1], 'b': [0, 1, 1]}, dtype=bool)
    df2 = pd.DataFrame({'a': [0, 1, 1], 'b': [1, 1, 0]}, dtype=bool)
@@ -720,7 +720,7 @@ Transposing
 To transpose, access the ``T`` attribute (also the ``transpose`` function),
 similar to an ndarray:
 
-.. ipython:: python
+.. code:: python
 
    # only show the first 5 rows
    df[:5].T
@@ -734,7 +734,7 @@ Elementwise NumPy ufuncs (log, exp, sqrt, ...) and various other NumPy functions
 can be used with no issues on Series and DataFrame, assuming the data within
 are numeric:
 
-.. ipython:: python
+.. code:: python
 
    np.exp(df)
    np.asarray(df)
@@ -748,7 +748,7 @@ array.
 
 The ufunc is applied to the underlying array in a Series.
 
-.. ipython:: python
+.. code:: python
 
    ser = pd.Series([1, 2, 3, 4])
    np.exp(ser)
@@ -762,7 +762,7 @@ Like other parts of the library, pandas will automatically align labeled inputs
 as part of a ufunc with multiple inputs. For example, using :meth:`numpy.remainder`
 on two :class:`Series` with differently ordered labels will align before the operation.
 
-.. ipython:: python
+.. code:: python
 
    ser1 = pd.Series([1, 2, 3], index=['a', 'b', 'c'])
    ser2 = pd.Series([1, 3, 5], index=['b', 'a', 'c'])
@@ -773,7 +773,7 @@ on two :class:`Series` with differently ordered labels will align before the ope
 As usual, the union of the two indices is taken, and non-overlapping values are filled
 with missing values.
 
-.. ipython:: python
+.. code:: python
 
    ser3 = pd.Series([2, 4, 6], index=['b', 'c', 'd'])
    ser3
@@ -782,7 +782,7 @@ with missing values.
 When a binary ufunc is applied to a :class:`Series` and :class:`Index`, the Series
 implementation takes precedence and a Series is returned.
 
-.. ipython:: python
+.. code:: python
 
    ser = pd.Series([1, 2, 3])
    idx = pd.Index([4, 5, 6])
@@ -801,19 +801,19 @@ You can also get a summary using :meth:`~pandas.DataFrame.info`.
 (Here I am reading a CSV version of the **baseball** dataset from the **plyr**
 R package):
 
-.. ipython:: python
+.. code:: python
    :suppress:
 
    # force a summary to be printed
    pd.set_option('display.max_rows', 5)
 
-.. ipython:: python
+.. code:: python
 
    baseball = pd.read_csv('data/baseball.csv')
    print(baseball)
    baseball.info()
 
-.. ipython:: python
+.. code:: python
    :suppress:
    :okwarning:
 
@@ -823,21 +823,21 @@ R package):
 However, using ``to_string`` will return a string representation of the
 DataFrame in tabular form, though it won't always fit the console width:
 
-.. ipython:: python
+.. code:: python
 
    print(baseball.iloc[-20:, :12].to_string())
 
 Wide DataFrames will be printed across multiple rows by
 default:
 
-.. ipython:: python
+.. code:: python
 
    pd.DataFrame(np.random.randn(3, 12))
 
 You can change how much to print on a single row by setting the ``display.width``
 option:
 
-.. ipython:: python
+.. code:: python
 
    pd.set_option('display.width', 40)  # default is 80
 
@@ -845,7 +845,7 @@ option:
 
 You can adjust the max width of the individual columns by setting ``display.max_colwidth``
 
-.. ipython:: python
+.. code:: python
 
    datafile = {'filename': ['filename_01', 'filename_02'],
                'path': ["media/user_name/storage/folder_01/filename_01",
@@ -857,7 +857,7 @@ You can adjust the max width of the individual columns by setting ``display.max_
    pd.set_option('display.max_colwidth', 100)
    pd.DataFrame(datafile)
 
-.. ipython:: python
+.. code:: python
    :suppress:
 
    pd.reset_option('display.width')
@@ -872,7 +872,7 @@ DataFrame column attribute access and IPython completion
 If a DataFrame column label is a valid Python variable name, the column can be
 accessed like an attribute:
 
-.. ipython:: python
+.. code:: python
 
    df = pd.DataFrame({'foo1': np.random.randn(5),
                       'foo2': np.random.randn(5)})
-- 
2.21.0

